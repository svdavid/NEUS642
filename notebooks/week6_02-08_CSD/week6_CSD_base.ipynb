{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8549357d",
   "metadata": {},
   "source": [
    "# Current Source Density Analysis \n",
    "\n",
    "## How can we highlight local aspects of electrophysiology data in the presence of volume conduction? Can we identify the laminar structure of the auditory cortex given a laminar array of electrodes and auditory stimuli?\n",
    "\n",
    "Individual neurons act as sources and sinks of current forming dipole moments. Depending on the anatomy of the given brain region being recorded from, neurons are often aligned forming larger cumulative dipoles that are picked up by the recording electrode. If the dipole moment for two neurons are not aligned this can effectively cancel out. Fortunately, cortex has fairly regular laminar structure with many neurons having similar alignments. When a neuron receives input at its dendrites and channels open allowing positively chareged ions in this leads to a negative deflection in the extracellularly recorded potentials (called a current sink), and the extracellular space farther away from this sink will be more positive and will be a source for return current (current source). <i>\n",
    "\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"images/sources and sinks.png\" alt=\"drawing\" width=\"500\"/></p>\n",
    "\n",
    "\n",
    "The activity at recording electrodes reflects a mixture of local activity and activity from neighboring sources that can propogate via the conductive media of the brain. We want to reduce this volume conducted signal to identify local positive (sources) and negative (sinks) in the LFP signal which can reflect localized neural activity. To address this issue, we can simply take the spatial derivative, the change in voltage per distance, given the known spacing of contact sites. This is typically called current source density analysis when applied to multicontanct probes in the brain and has been used to identify local activity patterns associated with laminar processing. For a more thorough background, see Neuromethods (2012) 67: 205â€“218 DOI 10.1007/7657_2011_6. \n",
    "    \n",
    "<p style=\"text-align: center;\"><img src=\"images/laminar_probe_CSD.png\" alt=\"drawing\" align=\"center\" width=\"500\"/></p>\n",
    "\n",
    "We will now go through some of the basic signal processing steps needed to estimate the CSD for laminar probes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917f7d94",
   "metadata": {},
   "source": [
    "### Function list\n",
    "\n",
    "Numpy\n",
    "* `np.sin()\n",
    "*  np.pi()\n",
    "*  np.hanning()\n",
    "\n",
    "Matplotlib (these are not central to today's presentation but are good to know about). We'll mostly be using plot functions that we already know.\n",
    "* `plt.subplots`\n",
    "* `plt.errorbar`\n",
    "\n",
    "Scipy\n",
    "* `scipy.signal.butter()\n",
    "*  scipy.signal.sosfilt()\n",
    "*  scipy.signal.convolve2d()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d763fb",
   "metadata": {},
   "source": [
    "# Load the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce2e412",
   "metadata": {},
   "source": [
    "First, import the requisite libraries. You should already have all these installed in your Anaconda environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baee97a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as scipy\n",
    "import tables as tb\n",
    "from scipy.signal import convolve2d, butter, sosfilt\n",
    "import pycurl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65718cd8-1284-4b09-b717-953b8bbe4103",
   "metadata": {},
   "source": [
    "## Use pycurl to pull down the data\n",
    "\n",
    "pycurl is cool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b776d6-c4db-4c1b-bcc8-4c9969ca83e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As long as the file is opened in binary mode, both Python 2 and Python 3\n",
    "# can write response body to it without decoding.\n",
    "with open('data/TAR010a.h5', 'wb') as f:\n",
    "    c = pycurl.Curl()\n",
    "    c.setopt(c.URL, 'https://hearingbrain.org/tmp/TAR010a.h5')\n",
    "    c.setopt(c.WRITEDATA, f)\n",
    "    c.perform()\n",
    "    c.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33df572d",
   "metadata": {},
   "source": [
    "## Exercise 1 - load the data\n",
    "\n",
    "Raw (1500Hz lowpass filtered and downsampled for sake of convenience)- data from a 64 channel silicone probe recording made in ferret primary auditory cortex neurons has been stored in a .hdf5 file in the `data/` subdirectory. This is the same general file type as the data from week 4 image processing and can be accessed with the tables module, tb. This file should contain the raw voltage data and epoch timestamps surrounding the stimuli.\n",
    "\n",
    "Remember, the tb module has a function, open_file, that will let us store a \"File\" object needed to access the data. Once we have a file handle object we can use the print function to get some basic information about the directories it contains. Using the directory information obtained with the print function, try to load the data from the probe into 2 separate variables called \"probedata\", \"epochs\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00b4a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load file\n",
    "fh = tb.open_file('data/TAR010a.h5')\n",
    "\n",
    "#print file directories\n",
    "print(fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd9b25f-a2b5-48d0-8dfd-a1c2df8998f3",
   "metadata": {},
   "source": [
    "Look at the output above, use the `get_node` command to load probedata... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afbe4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "node = fh.get_node('/probedata')\n",
    "probedata = node.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c14155b-262b-42d8-b17b-85238404c417",
   "metadata": {},
   "source": [
    "Check to make sure you've got the right shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745b37e7-6a7f-40c5-bf0a-bf029b1b9993",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(probedata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30e3000",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load trial epochs\n",
    "node = fh.get_node('/epochs')\n",
    "epochs = node.read()\n",
    "print(epochs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0c4ceb",
   "metadata": {},
   "source": [
    "# Signal processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdfe35c",
   "metadata": {},
   "source": [
    "Often times, we have data that we know has multiple unique sources in it, noise, or components that are not of interest given a particular question. In extracellular electrophysiology data there are often multiple components we are interested in, AP/spikes - voltage fluctaions on the order of 1-2ms due to fast Na+/K+ channel dynamics and the local field potential (LFP) - thought to primarily reflect slower timescale channels (AMPA/GABA) as well as synchronous spiking near the electrode. Although somewhat arbitrary, we can typically separate these different features of interest by filtering the data. Here we will briefly go over signal filtering which is an important step in many signal processing pipelines.\n",
    "\n",
    "To demonstrate the basics of signal filtering, lets create two simple signals with different frequency characterstics and then use a filter to separate them again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ad1d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulation parameters\n",
    "freq1 = 10\n",
    "freq2 = 800\n",
    "fs = 1000\n",
    "time = np.arange(-1, 1, 1/fs)\n",
    "\n",
    "#create signals\n",
    "signal1 = np.sin(2*np.pi*freq1*time)\n",
    "signal2 = np.sin(2*np.pi*freq2*time)\n",
    "\n",
    "#combine signals\n",
    "combined = signal1 + signal2\n",
    "\n",
    "#plot combined signal\n",
    "plt.plot(time, combined)\n",
    "plt.plot(time, signal2)\n",
    "plt.plot(time, signal1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbc9793",
   "metadata": {},
   "source": [
    "Lets use a filter to separate the two known sources.\n",
    "\n",
    "A butterworth filter is a commonly used filter that has a flat profile in the pass band, the frequencies that it lets\n",
    "through won't have amplitude distortions or ringing effects that some other filter kernels create. Scipy has a nice function that lets us design our own butterworth filter.\n",
    "\n",
    "The major parameters to be considered are the filter order, which is essentially the length of our filter kernel. This parameter specifies length based on a multiple of the the lowest frequency asked for in the cutoff frequency. In general, this should be larger than 3 which is the default. The larger the filter order, the better the filter frequency precision although this comes at a drawback of increased computation time and creates a shift in the phase of the signal (this can be corrected for so it is not a big deal). The other parameters are the cutoff frequencies/passband which specifies the frequencies that we want to filter out of our signal, the filter type (low, high, bandpass), and the sampling frequency. One common way to implement a filter is a second order sections method which we implement here.\n",
    "\n",
    "For more information regarding filters, I'd recommend Mike X Cohens book analyzing neural time series which has a whole chapter devoted to properly designing filters. Additonally, The scientists and engineers guide to digital signal processing is freely available although a bit more dense and less fun https://www.dspguide.com/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb03752e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter parameters\n",
    "order = 5\n",
    "passband = 50\n",
    "\n",
    "#design filter\n",
    "sos = scipy.signal.butter(order, passband, 'lowpass', fs=fs, output='sos')\n",
    "\n",
    "#filter data\n",
    "signal1filt = scipy.signal.sosfilt(sos, combined)\n",
    "\n",
    "#plot signals\n",
    "plt.figure()\n",
    "plt.plot(time, combined)\n",
    "plt.plot(time, signal1filt)\n",
    "plt.plot(time, signal1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac9a806",
   "metadata": {},
   "source": [
    "This example is extremely basic, but it shows that our filter can essentialy extract one signal from a combined signal. Which is pretty cool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4315bb2",
   "metadata": {},
   "source": [
    "## Exercise 2 - Create a filter to isolate the higher frequency signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906b4a66",
   "metadata": {},
   "source": [
    "Now that you have some experience with filtering, design a filter to extract the high frequency component of the combined signal we generated and save it as \"signal2filt\". Then plot the first 100 samples of your filter signal and the original high frequency signal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d013ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 5\n",
    "passband = 50\n",
    "sos = scipy.signal.butter(order, passband, 'highpass', fs=fs, output='sos')\n",
    "\n",
    "signal2filt = scipy.signal.sosfilt(sos, combined)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(time[:100], signal2filt[:100])\n",
    "plt.plot(time[:100], signal2[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6becb1d",
   "metadata": {},
   "source": [
    "## Exercise 3 - Filter raw data to extract the local field potential and the multiunit activty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2a0685",
   "metadata": {},
   "source": [
    "As stated earlier, the data has been lowpass filtered and downsampled below 1500Hz. This means the signal should contain the low frequency signal neuroscientists typically refer to as the local field potential as well as high frequency spiking related activity although the sampling rate and filter are much too low to identify spike waveforms we will use this activity as a measure of what is typically refered to as multiunit activity or general spike related activity. \n",
    "\n",
    "Given our experience with butterworth filters, lets filter the data to create a signal called \"mua\" that contains data high-pass filtered above 100Hz. Lets also save the local field potential activity in a variable called \"lfp\" that is bandpassfiltered between 1 and 150Hz. It is worth noting that our data currently has multiple dimensions and can be filtered along any of them. We want to filter along the time dimension\n",
    "so we will need to use an additional input in sosfilt to specify the axis we want to filter along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d76d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter mua\n",
    "fs = 1500\n",
    "sos = scipy.signal.butter(4, 100, 'highpass', fs=fs, output='sos')\n",
    "mua = scipy.signal.sosfilt(sos, probedata, axis=1)\n",
    "\n",
    "\n",
    "#filter lfp\n",
    "sos = butter(4, [1, 150], 'bandpass', fs=fs, output='sos')\n",
    "lfp = sosfilt(sos, probedata, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eb7536",
   "metadata": {},
   "source": [
    "Spiking activity has negative and positive deflections and we want to convert this into an amplitude profile for cleaner viewing and interpretation. To do this, we will take the absolute value of this signal with numpy's abs function. np.abs(data) will take the absolute value of the input. Try this for the filtered mua. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477ccdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract mua amplitude\n",
    "mua = np.abs(mua)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23311613",
   "metadata": {},
   "source": [
    "# Trial averaging the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d880e1",
   "metadata": {},
   "source": [
    "Although you can take CSD estimates for individual trials CSD is often estimated on trialized data which low pass filters the data to some extent creating a cleaner estimate. We have a list of tuples for each trial in the epochs data which has the start time of the trial epoch and the stop time of the trial epoch. The epochs contain a 0.5 second prestimulus silence followed by a tone and post stimulus silence period. Try to find out how long the epochs are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469fc860",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take the epoch end and subtract the epoch start to look at length\n",
    "print((epochs[:, 1] - epochs[:, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d084ebd9",
   "metadata": {},
   "source": [
    "## Exercise 4 - extracting event data and trial averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eb90bf",
   "metadata": {},
   "source": [
    "Given the knowledge about the epochs, use the first and last value in each row of the epochs data (the start and end of the trial epoch), to slice out these epochs for both mua and lfp. Check and make sure the ending dimensions match what is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b36867",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract stim events\n",
    "epochlen = int(1.6*1500)\n",
    "\n",
    "#convert epoch times to sample numbers\n",
    "epochs = epochs*fs\n",
    "\n",
    "#make a trial list\n",
    "lfp_trials = np.zeros((60, 64, epochlen))\n",
    "mua_trials = np.zeros((60, 64, epochlen))\n",
    "#loop through list of epochs and for each epoch extract \n",
    "for i in range(60):\n",
    "    lfp_trials[i, :] = lfp[:, round(epochs[i, 0]):round(epochs[i, 1])]\n",
    "    mua_trials[i, :] = mua[:, round(epochs[i, 0]):round(epochs[i, 1])]\n",
    "    \n",
    "#sanity check that data shape is correct\n",
    "np.shape(lfp_trials),np.shape(mua_trials)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09599e4a",
   "metadata": {},
   "source": [
    "Take the mean of both the mua and the lfp over all 60 trials and plot the first channel of averaged data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeedb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trial average the data along the trials dimension\n",
    "\n",
    "lfp_tavg = lfp_trials.mean(axis = 0)\n",
    "mua_tavg = mua_trials.mean(axis = 0)\n",
    "\n",
    "plt.plot(lfp_tavg[0, :])\n",
    "plt.plot(mua_tavg[0, :])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183dd6d5",
   "metadata": {},
   "source": [
    "# Taking a single column of electrodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fb85af",
   "metadata": {},
   "source": [
    "The method of CSD estimation that we are using is easiest to implement for uniform grids. The probes in this dataset have an unique arrangement with 3 columns of uniformly spaced electrodes that are 50 microns apart. We'll extract a single uniformly spaced column and estimate the CSD from that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1838e0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here are the channel numbers for each of the the three columns of electrodes\n",
    "left_ch_nums = np.arange(3,64,3)-1\n",
    "right_ch_nums = np.arange(4,65,3)-1\n",
    "center_ch_nums = np.insert(np.arange(5, 63, 3),obj=slice(0,1),values =[1,2],axis=0)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64955538",
   "metadata": {},
   "source": [
    "Using the channel maps provided above slice out the center column of electrodes and save as \"lfp_tavg_cent\" and \"mua_tavg_cent\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3feee1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lfp_tavg_cent = lfp_tavg[center_ch_nums]\n",
    "mua_tavg_cent = mua_tavg[center_ch_nums]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7450ee",
   "metadata": {},
   "source": [
    "In this particular case, we are also going to apply a spatial hanning window filter to smooth the data between electrodes. To do this, we generate a hanning window, a tapered function, and by applying this taper across the channels each point becomes a weighted sum of itself and its neighbors. This should smooth or decrease channel to channel variability that may show up in CSD which emphasizes local differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a029446",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using np.hanning we can create a hanning window to use to smooth our data. np.hanning takes one input which determines the number of samples used\n",
    "#to create the window. We will use 5 for this which will mean we are estimating each electrode given two contacts on either side.\n",
    "\n",
    "spatial_filter = np.hanning(5)[:, np.newaxis]\n",
    "\n",
    "#We will scale it by the sum of the filter so that the total value of the filter is 1. \n",
    "spatial_filter = spatial_filter / spatial_filter.sum()\n",
    "\n",
    "#convolution will take the sliding dot product of our filter with our data which smooths the data by estimating each point as the weighted sum of its neighbors.\n",
    "#the result of convolution can be forced to be the same size as the first input with mode = same.\n",
    "CSD_formatted = convolve2d(lfp_tavg_cent, spatial_filter, mode='same', boundary='symm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40a3f4e",
   "metadata": {},
   "source": [
    "# Current source density algorithm - estimating the second spatial derivative\n",
    "\n",
    "The second spatial derivative is often approximated by a three point formula involving taking the difference in voltage over space between two pairs of points - (voltage(x + some distance) - voltage(x)) - ((voltage(x) - voltage(x - some distance)). This is then normalized by the square of the distance - (some distance)^2. The distance chosen is a parameter that can vary and may impact the spatial resolution of identified sources and sinks. In the example from Higley, Neuromethods (2020), delta v is 200 microns as they choose to skip the nearest contact site in a probe with 100 micron spacing. This value may vary depending on your recording setup and anatomical considerations. Here we will just set the distance to one contact site.\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"images/CSD equation.png\" alt=\"drawing\" width=\"500\"/></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153fddd6",
   "metadata": {},
   "source": [
    "From the above equations, it should be apparrent that we will not be able to estimate the CSD at top or bottom electrode because we don't have a point above or below these electrodes respectively to perform the calculation. Although dubious, it is often the case that groups will extend/duplicate the data from the top and bottom electrodes under the assumption that the voltage recorded at nearby electrodes is not drastically different and therefore will not greatly impact the estimation. We will avoid doing this here and will simply lose data on both of these channels.\n",
    "\n",
    "Try to convert the equation from up above into code with the channel spacing, detla V, set to 50 microns. Also instead of skipping the neighboring site in our estimation lets use the nearest neighbor. Normalize by the square of the delta V."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7a04f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delta z\n",
    "ch_spacing = 50\n",
    "\n",
    "#initialize a vector of expected output\n",
    "CSD_est = np.zeros_like(lfp_tavg_cent[:20, :])\n",
    "\n",
    "#loop through each channel and estimate the CSD along the time dimension\n",
    "for i in range(0, 20):\n",
    "    V0 = CSD_formatted[i + 1, :]\n",
    "    Va = CSD_formatted[i, :]\n",
    "    Vb = CSD_formatted[i + 2, :]\n",
    "    CSD_est[i, :] = (Vb + Va - 2*V0)/(ch_spacing)**2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6ac4f6",
   "metadata": {},
   "source": [
    "Plotting of CSD is often done with the sign of values flipped so that sinks appear as positive warmer and sources appear as blue. Try to flip the sign of all the values in CSD_est."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6dc244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#flip sign of CSD for plotting purposes\n",
    "CSD_est = -CSD_est"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da31a44d",
   "metadata": {},
   "source": [
    "# Plot the estimated CSD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f2d306",
   "metadata": {},
   "source": [
    "Try using plt.imshow() to plot the CSD_est we just generated. Remember, imshow defaults to the origin being in the upper right which has be corrected with an additional parameter. In this case, the units of the x and y axis are not the same so try to find a way to adjust the aspect ratio to keep the axes fixed as this parameter defaults to 1 as it assumes we want square pixels as in images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e6bd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(CSD_est, origin = 'lower', aspect = 'auto', cmap = 'bwr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98365415",
   "metadata": {},
   "source": [
    "This looks good. However, we are plotting more data than necessary. The majority of the interesting components happen right around sample 750. This corresponds to the stimulus following a 0.5 second delay. Lets look at 0.05 seconds prestimulus and 0.15 seconds poststimulus. Try to slice and plot the data around this timeframe to see if it looks better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830cb1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define time for plotting\n",
    "prestim = 750/1500\n",
    "poststim = 1650/1500\n",
    "time = np.arange(-prestim, poststim, 1/fs)\n",
    "mask = (time > -.05) & (time < 0.15)\n",
    "plt.figure()\n",
    "plt.imshow(CSD_est[:, mask], extent = [-.05, .15, 0, 1000], origin = 'lower', aspect = 'auto', vmin = - .001, vmax = .001, cmap = 'bwr')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d98000a",
   "metadata": {},
   "source": [
    "Great! But what, if anything, do these stimulus evoked sources and sinks tell us about neuroanatomy?\n",
    "\n",
    "\"Noise-evoked columnar CSD patterns were used to determine the location of the A1 recording channel. Two CSD signatures were\n",
    "used to identify L4: A brief current sink first occurs approximately 10 ms after the noise onset, which was used to determine the lower\n",
    "border of L4 (Kaur et al., 2005). A triphasic CSD pattern (sink-source-sink from upper to lower channels) occurs between 20 ms and\n",
    "50 ms, where the border between the upper sink and the source was used to define the upper boundary of L4. Normally, 2 channels\n",
    "were assigned to L4. Other layers were defined relative to the location of L4 (L2/3: 3 channels above L4; L5: 3 channels below L4; L6: 3\n",
    "channels below L5). CSD-derived layer assignments were cross-validated against sound-evoked MUA response patterns, where L4\n",
    "and L5 units responded with higher firing rates and shorter latency.\"\n",
    "(Guo et al 2017)\n",
    "\n",
    "Given this knowledge, lets place a vertical ine on our plot at 10ms which should be near the first sink in the signal and might help visualize it.\n",
    "\n",
    "Also, we have mua data that we filtered. Try to plot this overtop of the CSD plot. Our y axis in the CSD is related to electrode position in microns. Try to use the knowledge that each channel is spaced 50 microns appart as well as the fact that the mua signal is close to 0 mV at rest to separate out the channels based on spatial location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d63ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_map = np.arange(0, 20*50, 50)\n",
    "plt.figure()\n",
    "plt.imshow(CSD_est[:, mask], extent = [-.05, .15, 0, 1000], origin = 'lower', aspect = 'auto', vmin = - .001, vmax = .001, cmap = 'bwr')\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label(\"CSD uV/um^2\")\n",
    "for i in range(len(mua_tavg_cent)-2):\n",
    "    plt.plot(time[mask], (mua_tavg_cent[i+1, mask] + np.repeat(ch_map[i], len(mua_tavg_cent[i+1, mask]))), linewidth = 2, color = 'k')\n",
    "plt.axvline(x = 0.01, color = 'k', label = 'axvline - full height')\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Position from recording tip (microns)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
