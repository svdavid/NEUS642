{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# NEUS 642 - Week 2 In-class exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Note about formating functions\n",
    "\n",
    "If you're given an empty, placeholder function (aka, a \"stub\"), please use this for formatting your answer. This should help you organize your thoughts. \n",
    "\n",
    "Dummy question. Write a function that takes two inputs, a and b, and returns their sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_numbers(a,b):\n",
    "    # Your work here\n",
    "\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it -- will not work until you add the correct code inside the function\n",
    "add_numbers(1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Then leave the function definition as is and only fill in the insider. That way you can test it with the code provided. It also makes it much easier to grade the homework, since I can just use the same code to test everyone's self-contained function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_numbers(a,b):\n",
    "    # Your work here\n",
    "    c = a + b   # simply enter your code here, using the variable names provided\n",
    "    \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it -- now it will work.\n",
    "add_numbers(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# Starting conditions, from homework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "We'll continue working with the spike data from class. So first, let's initialize this instance of python and load the datasets, same as in the lecture notebook. Refer to the lecture notebook if you need to remind yourself about the formatting of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes1 = np.loadtxt('data/CRD016c-40-1.csv', delimiter=',')\n",
    "cell1_label = 'CRD016c-40-1'\n",
    "spikes2 = np.loadtxt('data/CRD016c-50-1.csv', delimiter=',')\n",
    "cell2_label = 'CRD016c-50-1'\n",
    "\n",
    "stim = np.loadtxt('data/stim.csv', delimiter=',')\n",
    "stim = np.round(stim, 3)\n",
    "print(\"Loaded spike times for two neurons and list of stimulus events\")\n",
    "print(\"spikes1 - number of spike events:\", spikes1.shape)\n",
    "print(\"spikes2 - number of spike events:\", spikes2.shape)\n",
    "print(\"stim    - number of stimuli X [freq, start, stop]\", stim.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# Plot a tuning curve with error bars\n",
    "\n",
    "The frequency tuning curve we generated in class plots the mean spike rate response to each distinct stimulus, but it doesn't indicate which of those *responses are signficantly different from the spontaneous spike rate*. We want to determine which stimulus frequencies evoke a response.\n",
    "\n",
    "**Statistics primer:** One quick way to assess significance is to measure the standard error on the mean (SEM), which is the standard deviation across a list of samples $(x)$, divided by the square root of the number of samples. Important detail for computing statistics on real data: estimating standard deviation and SEM requires a \"degree of freedom\" (DOF) correction, which means dividing by $n-1$ instead of $n$ (The basic intuition for this correction is that you need one DOF in your n data points to estimate the mean, $m$, so you only have $n-1$ left to compute variance/SEM):\n",
    "\n",
    "$SEM = \\frac{1}{\\sqrt{n-1}} \\sqrt{(\\sum_i^n{(x(i)-m)^2} / (n-1))} = \\frac{1}{n-1} \\sqrt{(\\sum_i^n{(x(i)-m)^2}}$\n",
    "\n",
    "As a simple rule of thumb, the measurement of a mean value is signficant with $p<0.05$ if it is at least **2 SEM** from baseline. \n",
    "\n",
    "**Another note/warning(!!)** SEM works if your data samples are *independent and normally distributed*. Because we randomly varied the order of stimulus presentation, we can pretty safely assume the samples are independent. \"Normal\" means the distribution can be described by a Gaussian function. Spiking activity isn't normal, but let's assume it is today. We'll learn more about non-parametric statistics, which are better for analyzing spiking and other biological data later.\n",
    "\n",
    "But for now, let's stick with SEM. Using `numpy`, we can compute SEM with the `np.std` function. Use the `ddof` parameter to include the degrees-of-freedom correction.\n",
    "\n",
    "So we will do three things:\n",
    "\n",
    "1. Compute the spontaneuous spike rate for a neuron as a measure of baseline.\n",
    "2. Compute the mean and SEM response to each tone (include 1-degree of freedom correction)\n",
    "3. Plot the mean response to each tone with error bars to indicate whether 2 SEM do or do not overlap with the spont rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "A bit more about SEM calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([4,4,4,5,5,5,6,6,6])\n",
    "n=len(x)\n",
    "std = np.std(x, ddof=1)\n",
    "sem = std / np.sqrt(n-1)\n",
    "print(\"SEM for\", len(x), \"samples:\", sem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Note that as we collect more samples, our SEM typically gets smaller, meaning we're more confident about our measurment of the mean. Standard deviation does not get smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = np.array([4,4,4,5,5,5,6,6,6,4,5,6,4,5,6,4,5,6])\n",
    "n=len(x2)\n",
    "sem = np.std(x2, ddof=1)/np.sqrt(n-1)\n",
    "print(\"SEM for\", len(x2), \"samples:\", sem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "There are many ways to skin a cat. You can compute SEM directly without the `std` function. And/or use the alternative formulation of `np.sqrt(x)` as an exponent, `x**0.5`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.mean(x2)\n",
    "sem = np.sum((x2-m)**2)**0.5/(n-1)\n",
    "print(\"Same thing:\", sem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Question 1 - Function to compute spontaneous rate.\n",
    "\n",
    "It's considered good practice to break down, or *modularize*, analysis code into small chunks, making it easier to debug and reuse. One particular place where modularization is helpful is between code that performs computations and code that plots the results.\n",
    "\n",
    "So let's start with something simple and bite-sized. To assess if a response is significant, we need to measure the change in spike rate relative to the spontaneous spike rate in units of spikes/sec. Let's write a function that calculates spont rate from our data. This should be easy, simply \"packaging\" code from the lecture inside a function like this:\n",
    "\n",
    "```\n",
    "spont_rate = calc_spont_rate(stim, spikes)\n",
    "```\n",
    "\n",
    "**Note** that the variable containing the spike data is called `spikes` (not `spikes1` or `spikes2`).  Why is that?  We also pass the `stim` variable to this function. Why is that imporant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_spont_rate(stim, spikes):\n",
    "    \"\"\"\n",
    "    Average spike rates across the pre-stimulus window for all stimulis \n",
    "    listed in stim and return the average spont rate in units of spikes/sec.\n",
    "    \"\"\"\n",
    "    # Your answer here\n",
    "\n",
    "    return spont_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Test it. This should produce the same answer as we got during lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_spont_rate(stim, spikes1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "And we can call the same function for the second neuron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_spont_rate(stim, spikes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Question 2 - Function to compute mean and SEM response for each tone\n",
    "\n",
    "Next step: write a function that takes our `spikes` and `stim` arrays, and returns three arrays: a list of unique stimulus frequencies, the mean response to each frequency and the SEM for each frequency. Again, you should be able to borrow a most of the code from the lecture notebook.\n",
    "\n",
    "Syntax:\n",
    "```\n",
    "f, m, sem = tuning_mean_sem(stim, spikes)\n",
    "\n",
    "```\n",
    "Notice, the template specifies that this function should return a tuple of THREE vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning_mean_sem(stim, spikes):\n",
    "    \"\"\"\n",
    "    Determine the list of unique stimulus frequencies and then compute \n",
    "    the mean and SEM spike rate for each frequencies (as in class, computed \n",
    "    across the time period from stimulus onset to offset).\n",
    "    \"\"\"\n",
    "    # Your answer here\n",
    "\n",
    "    return f, m, sem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Test it with data from the first unit (`spikes1`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, m, sem = tuning_mean_sem(stim, spikes1)\n",
    "\n",
    "i = 6\n",
    "print(f\"For {f[i]} Hz stimulus, mean response is {m[i]:.3f} spikes/sec, and SEM is {sem[i]:.3f} spikes/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the response to the stimulus closest to 1000 Hz\n",
    "flookup = 1000\n",
    "fmatch = np.argmin(np.abs(f-flookup))\n",
    "print(f\"For {f[fmatch]} Hz stimulus, mean response is {m[fmatch]:.3f} spikes/sec, and SEM is {sem[fmatch]:.3f} spikes/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## Question 3 - Make a pretty plot with error bars\n",
    "\n",
    "Finally: using the results generated by your two functions, write a function that generates a tuning curve with error bars to indicate stimuli that produce a significant response. The error bars in your plot should indicate **2 SEMs**. Points where the error bars do not cross the spont line are signficant responses.\n",
    "\n",
    "The function `plt.errorbar` should come in handy. Important syntax detail: `errorbar` requires three inputs. `errorbar(x,y,e)`, where x indicates values on the x axis, in this case, frequency.\n",
    "\n",
    "Don't forget to label your x and y axes with accurate and interpretable units. And use `cell_label` to give your plot a title. Syntax:\n",
    "```\n",
    "plot_mean_sem(f, m, sem, cell_label)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_mean_sem(f, m, sem, cell_label):\n",
    "    # Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "Test your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "spont = calc_spont_rate(stim, spikes1)\n",
    "f, m, sem = tuning_mean_sem(stim, spikes1)\n",
    "\n",
    "plot_mean_sem(f, m, sem, cell1_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "Now let's enjoy the satisfaction of nice modular code that can be applied to data from a second neuron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "spont = calc_spont_rate(stim, spikes2)\n",
    "f, m, sem = tuning_mean_sem(stim, spikes2)\n",
    "\n",
    "plot_mean_sem(f, m, sem, cell2_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "# Bonus question\n",
    "\n",
    "Look pretty! But interesting, are the error bars convincing? Remember the point about assuming a normal distribution from above. Any thoughts on why/how the error bars may not be accurate? Don't worry if you don't have ideas. This is a subtle but important aspect of data analysis that will come up again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your thoughts here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
