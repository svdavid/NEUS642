{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "<a href=\"#Overview\"></a>\n# Overview\n* <a href=\"#cc6e70f3-136c-4577-9ff1-e06161587837\">Week 4: Work with confocal image data</a>\n  * <a href=\"#7a94433b-d143-4e31-9225-81f3b4cde86b\">Introduction</a>\n  * <a href=\"#f48a28f7-ae73-4f51-9946-07b3e567bf73\">The data format</a>\n  * <a href=\"#9a053838-dcd0-4c2d-8f6f-aa989fb5198a\">The data</a>\n  * <a href=\"#6cf39d01-d650-4b2c-8e7d-81a28a1d89d6\">The problem</a>\n    * <a href=\"#94c9fe20-f0f7-4a03-906c-d52eae4c7034\">Function list</a>\n  * <a href=\"#a15b790a-7efe-410a-8fb3-054af2cbd97f\">Loading the data</a>\n  * <a href=\"#0c50bb2d-7532-4547-a2e0-a1996fed3cc2\">Indexing Numpy arrays and plotting them</a>\n    * <a href=\"#4f3197a6-4574-4f57-b0d4-bb53237b6982\">Review - extracting a range from a numpy array</a>\n    * <a href=\"#975bb79d-5632-4a7e-8819-c463580fc65b\">Review - mean along one axis</a>\n    * <a href=\"#d1a91b7d-5332-44dd-96f8-f7292f16f56b\">Review - call mean as a method</a>\n    * <a href=\"#b0b9debd-289f-4a58-9c63-3539143cb1aa\">Exercise 1: calculate the maximum projection</a>\n    * <a href=\"#3457f1e6-9703-4da2-9121-d173befb432c\">Exercise 2: Cropping the dataset and plotting it</a>\n    * <a href=\"#d174145a-03fb-4aac-b084-63bcffe17eaf\">Exercise 3: Using the documentation to figure out how to modify plotting behavior</a>\n  * <a href=\"#e6d523c2-d897-4418-9bf9-419b7dcac1f9\">Using saved statistics to analyze the image</a>\n    * <a href=\"#46931302-f562-4638-87b1-3210f76b4266\">Exercise 4: overlaying a scatterplot on the image</a>\n    * <a href=\"#68b849d5-4559-4b58-b7b3-1bdcd2f8e739\">Exercise 5: extract a fixed volume relative to puncta coordinates</a>\n    * <a href=\"#8c457068-50ef-43d4-a871-8fa964fc1d11\">Exercise 6: repeat for the receptor signal</a>\n  * <a href=\"#e71d71c9-2531-4a56-b4da-157b2b21ecb4\">Quantifying the receptor signal</a>\n    * <a href=\"#09fc82c3-5f82-484b-9376-5f97c4d8fc4f\">Exercise 7: Calculate mean receptor signal near each puncta</a>\n    * <a href=\"#4242e30d-b3dc-4104-8130-ddcbb4a82103\">Exercise 8: plot histogram of receptor signal and count number of functional synapses</a>\n  * <a href=\"#d0e19df4-e714-449a-9aaf-366dc2200162\">Bonus - Creating a composite image</a>"}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"cc6e70f3-136c-4577-9ff1-e06161587837\"></a>\n# Week 4: Work with confocal image data\n<a href=\"#Overview\">Return to overview</a>\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"7a94433b-d143-4e31-9225-81f3b4cde86b\"></a>\n## Introduction\n<a href=\"#Overview\">Return to overview</a>\n", "\n", "Today, we'll work with three-dimensional numpy arrays. We'll also get into important ideas around working with data generated by third-party software and handling large datasets more generally. Unlike Pandas DataFrames, which are designed to work with tabular (two-dimensional) data, Numpy arrays are designed to work with multidimensional data. This makes them especially well suited for confocal datasets, which often contain three-dimensional image data.\n", "\n", "Thanks to Brad Buran and the original Python Neuroscience Bootcamp crew for putting together the original version of this notebook!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"f48a28f7-ae73-4f51-9946-07b3e567bf73\"></a>\n## The data format\n<a href=\"#Overview\">Return to overview</a>\n", "\n", "Imaris is a third-party program that facilitates analysis of data acquired from a confocal and saves it in an open-sorce format based on HDF5. **HDF5 stands for hierarchial data format version 5.** A number of programming languages have packages for reading this file format. There are two main packages for Python, `h5py` and `pytables`. We will use `pytables` today.\n", "\n", "You can get some more background on pytables <a href=\"https://www.pytables.org/usersguide/tutorials.html\">here</a>. Note that `pytables` is designed to work closely with pandas, but `h5py` is more widely used in the community. They are quite similar conceptually."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"9a053838-dcd0-4c2d-8f6f-aa989fb5198a\"></a>\n## The data\n<a href=\"#Overview\">Return to overview</a>\n", "\n", "Up to 25 auditory nerve fibers synapse onto individual inner hair cells in\n", "normal-hearing individuals. However, these synapses can be permanently lost due\n", "to aging, exposure to noise or ototoxic drugs.  In experiments that study\n", "hearing loss, we need a way of quantifying the number of synapses per inner\n", "hair cell.\n", "\n", "One approach is to dissect the cochlea out of the experimental animals and use\n", "whole-mount immunohistochemistry to label the tissue with antibodies for\n", "pre-synaptic ribbons (CtBP2), post-synaptic receptors (GluR2) and cytoskeleton\n", "(Myosin VIIa). In a second step, each antibody is tagged with a fluorescent dye\n", "that can be illuminated using a laser (much like how a black light can cause\n", "certain materials to glow).\n", "\n", "The distribution of these fluorescent dyes (which map to the underlying\n", "distribution of the proteins of interest) can be captured by taking a series of\n", "two-dimensional images at various depths in the tissue.  These images are then\n", "\"stacked\" to create a three-dimensional image known as a Z-stack (since the\n", "third dimension is commonly referred to as the Z-axis).\n", "\n", "For this exercise, the dataset has been trimmed down to a small subset showing\n", "only two inner hair cells (the full dataset is 0.5 GB in size) with CtBP2 (fig.\n", "1a) and GluR2 (fig. 1b).\n", "\n", "**For simplicity, we will refer to the CtBP2 label as \"ribbon\" and the GluR2 label as \"receptor\".**\n", "\n", "<table>\n", "\t<body>\n", "\t\t<tr>\n", "\t\t\t<td>1A. CtBP2 (pre-synaptic ribbon)</td>\n", "\t\t\t<td>1B. GluR2 (post-synaptic glutamate receptor)</td>\n", "\t\t</tr>\n", "\t\t<tr>\n", "\t\t\t<td><img src=\"data/CtBP2.png\" /></td>\n", "\t\t\t<td><img src=\"data/GluR2.png\" /></td>\n", "\t\t</tr>\n", "\t</body>\n", "</table>\n", "\n", "During confocal acquisition the signal intensity of each label is acquired as a separate **channel**. For each label, we have a separate set of three-dimensional image data representing the signal intensity for that label."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"6cf39d01-d650-4b2c-8e7d-81a28a1d89d6\"></a>\n## The problem\n<a href=\"#Overview\">Return to overview</a>\n", "\n", "This dataset was analyzed using Imaris to identify all ribbon puncta (white dots\n", "in fig. 2a). If you look closely at the composite (fig. 2b), you'll see that\n", "not all puncta have a glutamate receptor patch next to them (fig. 2b). \n", "\n", "<table>\n", "\t<body>\n", "\t\t<tr>\n", "\t\t\t<td>A. CtBP2 puncta</td>\n", "\t\t\t<td>B. CtBP2 puncta overlaid on GluR2</td>\n", "\t\t</tr>\n", "\t\t<tr>\n", "\t\t\t<td><img src=\"data/CtBP2+points.png\" /></td>\n", "\t\t\t<td><img src=\"data/CtBP2+GluR2+points.png\" /></td>\n", "\t\t</tr>\n", "\t</body>\n", "</table>\n", "\n", "A functional inner hair cell synapse requires both a pre-synaptic ribbon and a\n", "post-synaptic glutamate receptor. The next step in our analysis is to determine\n", "whether each ribbon puncta is near a receptor.\n", "\n", "One approach is to extract a fixed volume around each ribbon puncta (e.g., a 1um\n", "cube) and quantify the amount of receptor label in that volume. Let's get started!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"94c9fe20-f0f7-4a03-906c-d52eae4c7034\"></a>\n### Function list\n<a href=\"#Overview\">Return to overview</a>\n", "\n", "`pytables` (library `tables`) functions\n", "* `open_file`\n", "* `get_node`\n", "* `node.read`\n", "* `print`\n", "\n", "Assorted others\n", "* `plt.imshow`\n", "* `plt.hist`\n", "* `pd.describe`\n", "* `imaris` - custom library functions\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"a15b790a-7efe-410a-8fb3-054af2cbd97f\"></a>\n## Loading the data\n<a href=\"#Overview\">Return to overview</a>\n", "\n", "First, let's import a few modules we'll need. Remember that a Python module is basically a Python file (or collection of files) that contain useful functions that you can use in your code. \n", "\n", "Most of them are common third-party modules; for today, however, we have written a helper module (`imaris`) to extract some of the statistics (calculated by Imaris). You loaded these statistics from a comma-separated-values (CSV) file previously. We are now going to load it directly from the HDF5 file itself."]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": ["# Popular third-party libraries\n", "import matplotlib.pyplot as plt\n", "import tables as tb\n", "import numpy as np\n", "import pandas as pd\n", "\n", "# Custom module written for this exercise\n", "import imaris\n", "\n", "# This is a special \"magic\" command that can be used in Jupyter Notebooks.\n", "# It ensures that Matplotlib shows the plot below each cell (you won't see\n", "# a plot otherwise).\n", "%matplotlib inline\n", "\n", "# Set the default precision for printing data in Numpy arrays. This is handy for \n", "# preventing confusing XXe-N notation.\n", "np.set_printoptions(precision=4)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now, let's open the file using the `pytables` library. Even though the library\n", "is called `pytables`, it installs itself in Python as `tables` and we imported\n", "it as `tb`.\n", "\n", "Here, `tb` is a module and `open_file` is a function defined in the module. The\n", "`open_file` function takes the path to the file and returns a PyTables `File`\n", "object.\n", "\n", "You've already worked extensively with objects before. A `DataFrame` is an\n", "object. Even a simple integer is an object! Objects can be thought of as a\n", "collection of data (i.e., attributes on the object) and functions (i.e.,\n", "methods on the object) that operate on this data.\n", "\n", "Here, `fh` is a `File` object that knows where the data is stored on disk, but\n", "it hasn't actually loaded the data itself. That's a good thing. HDF5 files can\n", "be many gigabytes in size (most computers only have a few gigabytes of memory).\n", "However, it has methods that help you load the data you need."]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": ["fh = tb.open_file('data/confocal dataset.ims')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now what? If you've never worked with a particular type of object before, you\n", "can explore it a number of ways. We've already discussed two ways you can\n", "explore them. What are they?\n", "\n", "Another way is to just type `print(fh)`. What actually gets printed depends on\n", "how the developers of the library implement it for their particular object.\n", "Sometimes you don't get anything useful. Fortunately, the `PyTables` developers\n", "decided to produce useful output when you call the `print` function on a `File`\n", "object.\n", "\n", "Go ahead, try it."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(fh)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This is a lot of information. As you can see, it nicely illustrates the\n", "structure of an HDF5 file. Recall that HDF5 is short for \"hierarchial\n", "data format\"? You can see that the data in the file is organized into\n", "what looks like a file-path like structure (e.g., `/Scene8/Content/Points0`).\n", "Much like your computer filesystem, the data is organized into \"folders\" (known\n", "as \"groups\" in HDF5 parlance) and \"files\" (known as \"nodes\" in HDF5 parlance).\n", "It's almost like a filesystem within a file.\n", "\n", "We're interested in finding the actual three-dimensional image data. Let's look\n", "through the information shown above. Does anything jump out as a clue as to where the data might be stored in the file?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In confocal imaging, each label is acquired using a separate wavelength channel. At the\n", "bottom of the list we see several rows that mention `Channel 0`, `Channel 1`\n", "and `Channel 2`. This is most likely the data we need.\n", "\n", "However, the channels appear several times (under `ResolutionLevel 0` and\n", "`ResolutionLevel 1`). Which one do we want? Our intuition as a programmer tells\n", "us that Imaris likely generates the dataset at multiple resolutions and uses\n", "the appropriate resolution based on your zoom level. For quantitative analysis,\n", "we probably want the highest resolution level. \n", "\n", "Take another look at the list. You'll notice that at the end of each line\n", "there's an indicator in parenthesis (`Group`, `Array`, `CArray`).  different\n", "types of nodes in the HDF5 file. \n", "\n", "Now, let's look at the `Channel 0/Data` line for each resolution level. There's\n", "some information about the size of the array. This tells us that\n", "`ResolutionLevel 0` contains the highest resolution data and `ResolutionLevel 1` \n", "contains the lowest resolution data. Otherwise, they should be identical."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"0c50bb2d-7532-4547-a2e0-a1996fed3cc2\"></a>\n## Indexing Numpy arrays and plotting them\n<a href=\"#Overview\">Return to overview</a>\n", "\n", "Let's take a look at `Channel 0` so we can understand how to work with\n", "the data. Based on the acquisition settings, channel 0 is the ribbon label and channel 1 is the receptor label. Let's start with the ribbon label.\n", "\n", "The file object has a method, `get_node` that returns a node object. This node object provides a method, `read`, that loads the data from disk and returns it as a Numpy array object.\n", "\n", "The array object has a `shape` attribute that tells you the size of the array."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["node = fh.get_node('/DataSet/ResolutionLevel 0/TimePoint 0/Channel 0/Data')\n", "data_ribbon = node.read()\n", "print(data_ribbon.shape)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Note that there are three numbers. The reflects the fact that we're working with three-dimensional image data. Since we're working with image data, each element in the array represents a voxel (i.e., a 3D pixel). Here's a way to visualize a 3D array:\n", "\n", "<img src=\"data/array_colour_slices.png\" />\n", "\n", "**Helpful note:** In the Imaris data, the first dimension is the Z-axis, second dimension the Y-axis and third (last) dimension the X-axis. The ordering of the dimensions is how Imaris saves the data, but another program may save the Z-axis as the last dimension. You can usually figure it out, and usually this information can be found somewhere in the documentation."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Say you want to pull out the pixel located at XYZ coordinates (60, 20, 50). Remember how to do this?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#Answer\n", "data_ribbon[50, 20, 60]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"4f3197a6-4574-4f57-b0d4-bb53237b6982\"></a>\n### Review - extracting a range from a numpy array\n<a href=\"#Overview\">Return to overview</a>\n", "\n", "If you want to extract a *range* of values, you can use the slice syntax. How do you extract the first two elements from a list? Do it for `x_list`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x_list = ['a','b','c','d']"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#Answer\n", "x_list[:2]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's pull out three elements from each dimension (this gives us a total of 3 x 3 x 3 = 27 values). As shown in the output below the cell, it looks like a set of 3 x 3 arrays that have been stacked 3-high."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data_ribbon[50:53, 20:23, 60:63]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["There are ways to visualize 3D data in Python. However, these approaches are\n", "not readily available out of the box for Jupyter notebooks. Let's focus on simple 2D plotting\n", "instead. A common way of presenting confocal image stacks is to take the\n", "maximum projection along an an axis (i.e., dimension). Let's take the maximum\n", "projection along the first axis (i.e., Z-axis)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["In week 2, we talked about how you can calculate statistics along an axis of a Numpy array. Let's refresh our memory."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data_2D = np.random.rand(3, 2)\n", "print('Full dataset')\n", "print(data_2D)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["By convention, a two-dimensional array has two axes: the first running vertically downwards across rows (axis 0) and the second running horizontally across columns (axis 1).\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"975bb79d-5632-4a7e-8819-c463580fc65b\"></a>\n### Review - mean along one axis\n<a href=\"#Overview\">Return to overview</a>\n", "\n", "We can pass an axis argument to the Numpy statistics functions (e.g., `mean`, `std`, `mean`) indicating which axis to operate across. If we take the mean of axis 0 (e.g., `np.mean(data_2D, axis=0)`) what is the shape of the result? What about when taking the mean of axis 1?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#Answer\n", "print('\\nMean across axis 0')\n", "print(np.mean(data_2D, axis=0))\n", "\n", "print('\\nMean across axis 1')\n", "print(np.mean(data_2D, axis=1))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"d1a91b7d-5332-44dd-96f8-f7292f16f56b\"></a>\n### Review - call mean as a method\n<a href=\"#Overview\">Return to overview</a>\n", "\n", "In addition to using the `mean` function available through the Numpy module (called `np` in this notebook), there is also a `mean` method available through the array object that performs the same operation. Go ahead, try it!"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#Answer\n", "print(data_2D.mean(axis=0))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's take a look at a very simple 3D array with shape 4 x 3 x 2. When printing a 3D array, Numpy prints out each 2D section separately."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data_3D = np.random.rand(4, 3, 2)\n", "\n", "print('Full dataset')\n", "print(data_3D)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["What will the shape of the resulting array be if you take the mean of the first axis? Second axis? Third axis?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('\\nMean across axis 0')\n", "print(np.mean(data_3D, axis=0))\n", "\n", "print('\\nMean across axis 1')\n", "print(np.mean(data_3D, axis=1))\n", "\n", "print('\\nMean across axis 2')\n", "print(np.mean(data_3D, axis=2))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"b0b9debd-289f-4a58-9c63-3539143cb1aa\"></a>\n### Exercise 1: calculate the maximum projection\n<a href=\"#Overview\">Return to overview</a>\n", "\n", "Now, calculate the maximum projection, along the Z-axis, of the confocal image. The maximum projection is the max value along that axis. You can either use the function, `np.max`, or the object method `data_ribbon.max`.\n", "\n", "Save it to a variable called `projection`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#Answer\n", "projection = data_ribbon.max(axis=0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Note the shape of the projection. The Z-axis has been dropped, leaving us with a two-dimensional array that we can plot as an image."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('Shape of full dataset', data_ribbon.shape)\n", "print('Shape of projection', projection.shape)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now, let's plot this 2D projection. The `origin='lower'` argument to `plt.imshow` indicates that the data at `projection[0, 0]` should appear at the lower left corner of the axes instead of the default location (the upper left)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.imshow(projection, origin='lower')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"3457f1e6-9703-4da2-9121-d173befb432c\"></a>\n### Exercise 2: Cropping the dataset and plotting it\n<a href=\"#Overview\">Return to overview</a>\n", "\n", "It looks like the image has been \"padded\" with empty data by Imaris, making it a bit ugly to look at. Let's crop out that extra data. To do this, we need to find out what the actual image extents are in pixels. There is a way to do this by looking at the HDF5 file, but this is outside the scope of the exercise. For now, we provide the numbers for you."]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": ["x_pixels = 161\n", "y_pixels = 194\n", "z_pixels = 135"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Use Numpy indexing to crop out the valid `z_pixels` x `y_pixels` x `x_pixels` array. Test that your answer makes sense by plotting the maximum projection of that new array."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#Answer\n", "cropped_ribbon = data_ribbon[:z_pixels, :y_pixels, :x_pixels]\n", "projection = cropped_ribbon.max(axis=0)\n", "plt.imshow(projection, origin='lower')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"d174145a-03fb-4aac-b084-63bcffe17eaf\"></a>\n### Exercise 3: Using the documentation to figure out how to modify plotting behavior\n<a href=\"#Overview\">Return to overview</a>\n", "\n", "The units on the X and Y-axes are in pixels. Let's convert them to actual image dimensions (in microns). First, you need to know the actual dimensions (again this can be done by looking at the HDF5 file, but we provide the numbers for you). These are the the dimensions of the cropped dataset."]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": ["x_um = 22.7418\n", "y_um = 27.442\n", "z_um = 21.526"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now, remember how to get help on a function? Take a look at the documentation for `plt.imshow`. Any clues as to what arguments can be used to get `imshow` to properly map each pixel to it's spatial location in microns? As a bonus, be sure to label the X and Y axes too! You can use the `xlabel` and `ylabel` functions available through the matplotlib.pyplot module (`plt` in this notebook)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#Answer\n", "extents = (0, x_um, 0, y_um)\n", "plt.imshow(projection, origin='lower', extent=extents)\n", "plt.xlabel('Position X (\u03bcm)')\n", "plt.ylabel('Position Y (\u03bcm)')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"e6d523c2-d897-4418-9bf9-419b7dcac1f9\"></a>\n## Using saved statistics to analyze the image\n<a href=\"#Overview\">Return to overview</a>\n", "\n", "Now, we need to load the data about the ribbon puncta that were identified using Imaris. Specifically, we need to know the XYZ location of each puncta. A function from the `imaris` module called `load_node_stats` can be used to load this from the Imaris file. The function takes three arguments (the PyTables `File` object, the name of the label and the type of statistics to extract). "]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": ["stats_ribbon = imaris.load_node_stats(fh, 'CtBP2', 'point')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["stats_ribbon.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Another useful pandas method is `describe`:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["stats_ribbon.describe()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"46931302-f562-4638-87b1-3210f76b4266\"></a>\n### Exercise 4: overlaying a scatterplot on the image\n<a href=\"#Overview\">Return to overview</a>\n", "\n", "Our goal is to take the plot we created using `imshow` (with the axes showing the correct spatial location in microns) and overlay a scatterplot showing the location of each ribbon puncta identified by Imaris.\n", "\n", "You've already learned how to inspect the contents of a dataframe. Take a look at the dataframe. What type of information does it have? What are the units (e.g., pixels or microns)?\n", "\n", "Once you have figured out how to obtain the X and Y coordinates for each puncta, you can plot them using `plt.plot(x_coordinates, y_coordinates, 'r+')` (the `'r+'` specifies a red cross marker). Do the coordinates align with the puncta observed in the image?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#Answer\n", "extents = (0, x_um, 0, y_um)\n", "plt.imshow(projection, origin='lower', extent=extents)\n", "x_coordinates = stats_ribbon['X']\n", "y_coordinates = stats_ribbon['Y']\n", "\n", "plt.plot(x_coordinates, y_coordinates, 'r+')\n", "plt.xlabel('Position X (\u03bcm)')\n", "plt.ylabel('Position Y (\u03bcm)')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We want to use this data to extract a 1\u03bcm x 1\u03bcm x 1\u03bcm cube centered around each puncta. To do this we need to convert from microns to pixels. Since we know the dimensions in pixels and microns, we can calculate the size of each pixel."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x_size = x_um/x_pixels\n", "y_size = y_um/y_pixels\n", "z_size = z_um/z_pixels\n", "\n", "print('Pixel size (x-axis) {:.2f}'.format(x_size))\n", "print('Pixel size (y-axis) {:.2f}'.format(y_size))\n", "print('Pixel size (z-axis) {:.2f}'.format(z_size))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Each pixel along the X and Y axes are 0.14 microns and the Z axis is 0.16 microns. If we want to convert from microns to pixels, we can divide by the pixel size. This means that a 1\u03bcm x 1\u03bcm x 1\u03bcm cube is approximately 7 x 7 x 6 pixels in size (rounded to the nearest pixel). For simplicity, let's assume that the cube should be 7 x 7 x 7 pixels in size.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"68b849d5-4559-4b58-b7b3-1bdcd2f8e739\"></a>\n### Exercise 5: extract a fixed volume relative to puncta coordinates\n<a href=\"#Overview\">Return to overview</a>\n", "\n", "Now that you know how to convert from microns to pixels, let's pull out the first puncta in the dataframe and plot the maximum projection of the 1\u03bcm\u00b3 region centered around the puncta.\n", "\n", "If you don't remember how to extract the first row of the dataframe, take a look at day 2."]}, {"cell_type": "markdown", "metadata": {}, "source": ["First, one small caveat. When indexing Numpy arrays (and also Python lists), you must use integers. Since divison returns floating point values, you need to cast the result to an integer."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["result = 32/4\n", "type(result)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["int_result = int(round(result))\n", "type(int_result)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now, enter your solution below:\n", "* Convert coordinates stored in first row of dataframe to pixels. Don't forget to cast to an integer!\n", "* Extract cube from `data_ribbon`. Be sure to verify its size is 7 x 7 x 7.\n", "* Compute maximum projection along z-axis and plot it.\n", "* Ensure axes are labeled appropriately and reflect data coordinates, not pixel coordinates."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#Answer\n", "point = stats_ribbon.loc[0]\n", "\n", "x_px = int(round(point['X']/x_size))\n", "y_px = int(round(point['Y']/y_size))\n", "z_px = int(round(point['Z']/z_size))\n", "\n", "subset_ribbon = data_ribbon[z_px-4:z_px+3, y_px-4:y_px+3, x_px-4:x_px+3]\n", "projection_ribbon = subset_ribbon.max(axis=0)\n", "plt.imshow(projection_ribbon, origin='lower', extent=(0, 1, 0, 1))\n", "plt.xlabel('Position X (\u03bcm)')\n", "plt.ylabel('Position Y (\u03bcm)')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"8c457068-50ef-43d4-a871-8fa964fc1d11\"></a>\n### Exercise 6: repeat for the receptor signal\n<a href=\"#Overview\">Return to overview</a>\n", "\n", "Looks like we've adequately identified the cube we need. Now, let's load and crop the receptor data so we can plot the amount of receptor signal within this region as well."]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": ["node = fh.get_node('/DataSet/ResolutionLevel 0/TimePoint 0/Channel 1/Data')\n", "data_receptor = node.read()\n", "data_receptor = data_receptor[:z_pixels, :y_pixels, :x_pixels]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#Answer\n", "subset_receptor = data_receptor[z_px-4:z_px+3, y_px-4:y_px+3, x_px-4:x_px+3]\n", "projection_receptor = subset_receptor.max(axis=0)\n", "plt.imshow(projection_receptor, origin='lower', extent=(0, 1, 0, 1))\n", "plt.xlabel('Position X (\u03bcm)')\n", "plt.ylabel('Position Y (\u03bcm)')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"e71d71c9-2531-4a56-b4da-157b2b21ecb4\"></a>\n## Quantifying the receptor signal\n<a href=\"#Overview\">Return to overview</a>\n", "\n", "Looks like there's some receptor signal next to the ribbon signal. Great! Now how do we quantify this? Maybe we can just take the average intensity within this subset? How do we do this?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["mean_receptor_signal = subset_receptor.mean()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"09fc82c3-5f82-484b-9376-5f97c4d8fc4f\"></a>\n### Exercise 7: Calculate mean receptor signal near each puncta\n<a href=\"#Overview\">Return to overview</a>\n", "\n", "The next step is to loop through each row (i.e., puncta) in the dataframe and extract the mean receptor signal. This can then be saved back as a new column in the dataframe. We can loop through the rows using the `iterrows` method. On each cycle of the for loop, the `iterrows` method returns two values. The first value is the index of the row and the second value is the data in the row itself (stored in a dictionary-like format)."]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": ["#Answer\n", "signal = []\n", "for _, puncta in stats_ribbon.iterrows():\n", "    x_px = int(round(puncta['X']/x_size))\n", "    y_px = int(round(puncta['Y']/y_size))\n", "    z_px = int(round(puncta['Z']/z_size))\n", "    subset_receptor = data_receptor[z_px-4:z_px+3, y_px-4:y_px+3, x_px-4:x_px+3]\n", "    mean_receptor_signal = subset_receptor.mean()\n", "    signal.append(mean_receptor_signal)\n", "    \n", "# Here, we can save the receptor signal back to the statistics dataframe as a new column\n", "stats_ribbon['receptor'] = signal"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"4242e30d-b3dc-4104-8130-ddcbb4a82103\"></a>\n### Exercise 8: plot histogram of receptor signal and count number of functional synapses\n<a href=\"#Overview\">Return to overview</a>\n", "\n", "Now, let's plot a histogram of the receptor signal near each ribbon puncta. Look at the histogram. Are there any obvious outliers? Is there an obvious cutoff threshold? Based on this, how many functional synapses are there? \n", "\n", "If you're not sure how to plot a histogram. Take a look at the documentation for the `matplotlib.pyplot` module (referred to as `plt` in this notebook). Any particular functions jump out at you?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#Answer\n", "plt.hist(stats_ribbon['receptor'])\n", "mask = stats_ribbon['receptor'] > 10\n", "print(len(stats_ribbon.loc[mask]))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"d0e19df4-e714-449a-9aaf-366dc2200162\"></a>\n## Bonus - Creating a composite image\n<a href=\"#Overview\">Return to overview</a>\n", "\n", "In the above images, `imshow` is using a color map in which purple reflects the regions with no signal and yellow reflects regions with the most signal. But, what if we'd like to merge the three channels into a single image where red is mapped to the ribbon and green to the receptor. How can we do this? Let's take another look at the documentation for `imshow`."]}, {"cell_type": "markdown", "metadata": {}, "source": ["It looks `imshow` can take a 3D array where the last dimension maps to the three colors (i.e., `x[..., 0]` is red, `x[..., 1]` is green and `x[..., 2]` is blue). The documentation also warns that the values in the array must be in the range 0 ... 1 for this to work. Let's check that. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": ["data_ribbon = data_ribbon[:z_pixels, :y_pixels, :z_pixels]\n", "data_receptor = data_receptor[:z_pixels, :y_pixels, :z_pixels]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data_ribbon.max()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Uh oh. We need to fix that. The simplest way to coerce data to the range 0 ... 1 is to divide by the maximum value. Let's do this and check that we did OK."]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": ["norm_data_ribbon = data_ribbon/np.max(data_ribbon)\n", "norm_data_receptor = data_receptor/np.max(data_receptor)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["norm_data_ribbon.max()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Great. Now we need to make the 2D image for each color and then merge them into a 3D array. A list of 2D images can be stacked into a 3D array using Numpy's `dstack` function. We need to make a blank image for the blue color. The quickest way to do this is to use the `zeros_like` function from Numpy which will create an array of the same shape, but filled with zeros."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["projection_ribbon = norm_data_ribbon.max(axis=0)\n", "projection_receptor = norm_data_receptor.max(axis=0)\n", "projection_blank = np.zeros_like(projection_ribbon)\n", "\n", "data = [projection_ribbon, projection_receptor, projection_blank]\n", "projection = np.dstack(data)\n", "\n", "plt.imshow(projection)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"anaconda-cloud": {}, "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.11"}}, "nbformat": 4, "nbformat_minor": 4}