{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9613d890-1f2e-481b-8cef-da26341343b3",
   "metadata": {},
   "source": [
    "# Week 8: Spike timing analysis (??)\n",
    "\n",
    "Couple sentences describing scientific question/data -- recordings from ?? under ?? conditions. \n",
    "\n",
    "Then a couple sentences describing what this notebook is trying to accomplish. Load xx data, perform YY analysis\n",
    "\n",
    "Data was collected using XX system, which saves traces in .axgt format, which contains tab-delimmited data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e1b9a6-3fdd-41ae-899e-08998337a12b",
   "metadata": {},
   "source": [
    "Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3201b9-b0c0-43b4-a880-f66c6e4d264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "from scipy.signal import find_peaks\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b23933d-c1cf-4f77-b4e8-f88e8bbe3f93",
   "metadata": {},
   "source": [
    "# Inspect a single data file\n",
    "\n",
    "First we want to get a handle on our data files. The `os.walk()` command provides a list of all files contained in dirtory tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b65d592-943c-4016-877a-526ab8bfec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "startpath='Data/'\n",
    "for root, dirs, files in os.walk(startpath):\n",
    "    for f in files:\n",
    "        print(f\"{root}/{f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affd4bf6-8ed2-4cc4-a940-3d2e242e7e0b",
   "metadata": {},
   "source": [
    "TODO: Explain the file format, how do we know what columns to extract?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd3f61a-7117-4acd-9f18-632351a47dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_file='Data/Naive/Morphine/01232023_7.axgt'\n",
    "\n",
    "Test = np.loadtxt(example_file,skiprows=1, delimiter='\\t')\n",
    "\n",
    "time = Test[:, 0]\n",
    "depo_pA = Test[:, 4]\n",
    "\n",
    "depo_pA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f5789d-e6b3-4878-b9e7-5a9b2f4cfc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time,depo_pA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab11519-cff9-476b-99ff-afc7d3e3a2e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddb812ce-7073-4972-a276-ac638be99770",
   "metadata": {},
   "source": [
    "## Take a close look at the trace\n",
    "\n",
    "Goal is to XX, logic for choosing time window is YY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb4b53e-d9f6-4f1c-95b7-0db10ed39e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the time range (between 1 and 2 seconds)\n",
    "start_time = 1\n",
    "end_time = 2\n",
    "\n",
    "# Find the indices within the specified time range\n",
    "time_indices = np.where((time >= start_time) & (time <= end_time))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4237de-b8a9-4dc2-b1bc-f6e802439c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time[time_indices],depo_pA[time_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6f7e3b-cb1d-4e2f-bfa0-feca0b38bb48",
   "metadata": {},
   "source": [
    "## Count Spikes\n",
    "\n",
    "TODO Explain what's happening. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf64405e-f106-4f63-9a85-cb9a88069ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the threshold\n",
    "threshold = 0\n",
    "\n",
    "# Call find_peaks\n",
    "peaks, _ = find_peaks(depo_pA, height=threshold)\n",
    "\n",
    "print(peaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bbc968-e94d-469e-9217-3b777785b23e",
   "metadata": {},
   "source": [
    "To get a better sense of what find_peaks does, we can plot the peaks overlaid on the voltage trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435e1ea0-eac4-46b8-a8f4-bb529efbbebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data with peaks\n",
    "plt.plot(time, depo_pA, label='100 pA')\n",
    "plt.plot(time[peaks], depo_pA[peaks], 'r.', label='Peaks')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Experiment 4 Data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01566eb-4c57-4bdb-a1b5-d38d59c114dd",
   "metadata": {},
   "source": [
    "### Exercise - Zoom in to stimuluation time\n",
    "\n",
    "Plot the data zoomed in only over the 1-sec stimulation period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0c98fd-334e-4941-a9e0-9eb2ecb4e4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "# Plot the data with peaks\n",
    "plt.plot(time, depo_pA, label='100 pA')\n",
    "plt.plot(time[peaks], depo_pA[peaks], 'r.', label='Peaks')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Experiment 4 Data')\n",
    "plt.legend()\n",
    "plt.xlim(1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947a110c-839e-40eb-9aa6-28f1c9a63cb4",
   "metadata": {},
   "source": [
    "# Inter-spike interval analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0e4132-d13c-42a7-bc1a-95249115dfe2",
   "metadata": {},
   "source": [
    "## Measure ISIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b9d9bd-50f6-4139-9afa-bbdee8f705f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure when spikes happened\n",
    "spike_times = time[peaks]\n",
    "\n",
    "# Calculate inter-spike intervals\n",
    "inter_spike_intervals = np.diff(spike_times)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Inter-spike intervals: {inter_spike_intervals}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb4c290-6ded-4d9b-82c7-bf6caf15c048",
   "metadata": {},
   "source": [
    "## Compute interesting statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbb9897-8b44-4bc1-be9a-8599fbf7e1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_spike_events = len(spike_times)\n",
    "\n",
    "if num_spike_events > 0:\n",
    "    spike_frequency = num_spike_events / np.sum(inter_spike_intervals)\n",
    "else:\n",
    "    spike_frequency = np.nan\n",
    "\n",
    "onset_time = 1\n",
    "time_to_last_spike = spike_times.max()-start_time if num_spike_events > 0 else None\n",
    "\n",
    "print(f\"Spike count: {num_spike_events}\")\n",
    "print(f\"Frequency: {spike_frequency} spk/sec\")\n",
    "print(f\"Time to last spike: {time_to_last_spike} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e99093-fcd8-45be-9202-c560e7fe4ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6c4ded6-cb8d-4686-8d8f-ec1a844ad9b8",
   "metadata": {},
   "source": [
    "# Exercise - Function to measure ISIs in one file\n",
    "\n",
    "Write a function that takes in a file name and uses peak_times to count spikes in the 1-2 sec time window. Return a list of spike times in seconds\n",
    "\n",
    "```spike_times = get_spike_times(filename)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2ec621-f929-43ff-84d4-74289c37087d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "def get_spike_times(filename, threshold=0, start_time=1, end_time=2):\n",
    "    \n",
    "    data = np.loadtxt(filename, skiprows=1, delimiter='\\t')\n",
    "    \n",
    "    time = data[:, 0]\n",
    "    depo_pA = data[:, 4]\n",
    "    \n",
    "    valid_time = (time>=start_time) & (time<=end_time)\n",
    "    time = time[valid_time]\n",
    "    depo_pA = depo_pA[valid_time]\n",
    "    \n",
    "    peaks, _ = find_peaks(depo_pA, height=threshold)\n",
    "    spike_times = time[peaks]\n",
    "    \n",
    "    return spike_times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac02ef3a-8675-45f7-9472-f857262c3e49",
   "metadata": {},
   "source": [
    "Test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d439c6-5361-4acf-85d9-5ecad46dea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = \"Data/Naive/Morphine/01232023_7.axgt\"\n",
    "spike_times = get_spike_times(f)\n",
    "print(f\"Spike times for file {f}: {spike_times}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9f3657-6054-4079-ae36-786419033ec0",
   "metadata": {},
   "source": [
    "## Iterate over files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f870f6-659a-477d-bb81-ceeb7fbbd48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_parts = example_file.split('/')\n",
    "f = path_parts[3]\n",
    "parent = path_parts[2]\n",
    "grandparent = path_parts[1]\n",
    "print(f\"file: {f}\")\n",
    "print(f\"condition (parent folder): {parent}\")\n",
    "print(f\"animal group (grandparent folder): {grandparent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a893163-7ccd-4468-9c84-57bde8d52f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files = pd.DataFrame()\n",
    "\n",
    "startpath='Data/'\n",
    "counter = 0\n",
    "for root, dirs, files in os.walk(startpath):\n",
    "    for f in files:\n",
    "        if f.endswith('axgt'):\n",
    "            full_file = root+'/'+f\n",
    "            path_parts = root.split('/')\n",
    "            parent = path_parts[2]\n",
    "            grandparent = path_parts[1]\n",
    "            df_files.loc[counter,'filepath']=full_file\n",
    "            df_files.loc[counter,'group']=grandparent\n",
    "            df_files.loc[counter,'condition']=parent\n",
    "            df_files.loc[counter,'file']=f\n",
    "            counter += 1\n",
    "df_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0395fccf-784e-409a-8529-1b5f7e05c4ec",
   "metadata": {},
   "source": [
    "### Exercise - Compute statistics for all files\n",
    "\n",
    "Write code that iteraties through each row of `df_files`, measures spike times and computes the spikes statistics. Save the spike count, spike frequency and time to last spike in columns `spike_count`, `spike_frequency` and `time_to_last_spike`, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4b2112-d66c-4916-8916-00c4ec6eb01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, r in df_files.iterrows():\n",
    "    print(\"Processing:\", i, r['filepath'])\n",
    "\n",
    "    spike_times = get_spike_times(r['filepath'])\n",
    "    \n",
    "    inter_spike_intervals = np.diff(spike_times)\n",
    "    num_spike_events = len(spike_times)\n",
    "    \n",
    "    if num_spike_events > 0:\n",
    "        spike_frequency = num_spike_events / np.sum(inter_spike_intervals)\n",
    "    else:\n",
    "        spike_frequency = np.nan\n",
    "    \n",
    "    onset_time = 1\n",
    "    time_to_last_spike = spike_times.max() if num_spike_events > 0 else None\n",
    "\n",
    "    df_files.loc[i,'spike_count']=num_spike_events\n",
    "    df_files.loc[i,'spike_frequency']=spike_frequency\n",
    "    df_files.loc[i,'time_to_last_spike']=time_to_last_spike\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2751aae8-ede5-400a-99a5-5ee0ade9b6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac976d5-3431-4707-8725-41f94ec141e6",
   "metadata": {},
   "source": [
    "# Compute group statistics\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e8d73a-3d18-4f10-88fb-9dbc359b98f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97994d11-ec3b-4126-a1c9-f2511319659d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175ee2ff-6545-47cc-9c94-e57aa2fd1d62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca759dae-5258-4826-a750-2751a2990a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = sio.loadmat('Naive/07282023 naive/FR  002 Copy_MAT.mat')\n",
    "#Test = np.loadtxt('Naive/07282023 naive/FR  001 Copy Export.axgt',skiprows=1, delimiter='\\t')\n",
    "\n",
    "\n",
    "#time = Test[:, 0]\n",
    "#depo_pA = Test[:, 4]\n",
    "\n",
    "#depo_pA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62faf725-85bb-43d3-ad20-6c5022219ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the threshold\n",
    "threshold = 0\n",
    "\n",
    "# Define the time range (between 1 and 2 seconds)\n",
    "start_time = 1\n",
    "end_time = 2\n",
    "\n",
    "# Find the indices within the specified time range\n",
    "time_indices = np.where((time >= start_time) & (time <= end_time))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b00d6e-c992-4cec-bb8b-a2f1ff03f618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a counter for the occurrences\n",
    "occurrences = 0\n",
    "\n",
    "# Iterate over the indices and count occurrences\n",
    "in_spike = False\n",
    "for idx in time_indices:\n",
    "    if depo_pA[idx] > threshold and not in_spike:\n",
    "        # Entering a spike event\n",
    "        in_spike = True\n",
    "        occurrences += 1\n",
    "    elif depo_pA[idx] <= threshold and in_spike:\n",
    "        # Exiting a spike event\n",
    "        in_spike = False\n",
    "\n",
    "# Print the result\n",
    "print(f\"Number of spike events between {start_time} and {end_time} seconds: {occurrences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206df0f3-1893-4aa1-8247-d9e7b4e2c5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks, _ = find_peaks(depo_pA, height=threshold)\n",
    "\n",
    "# Plot the data with peaks\n",
    "plt.plot(time, depo_pA, label='100 pA')\n",
    "plt.plot(time[peaks], depo_pA[peaks], 'r.', label='Peaks')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Experiment 4 Data')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculate inter-spike intervals\n",
    "inter_spike_intervals = np.diff(time[peaks])\n",
    "\n",
    "# Print the result\n",
    "print(f\"Inter-spike intervals: {inter_spike_intervals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384203e8-9c67-4037-96af-ca83d13f3c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of spike events\n",
    "num_spike_events = len(peaks)\n",
    "\n",
    "# Calculate spike frequency\n",
    "spike_frequency = num_spike_events / np.sum(inter_spike_intervals)\n",
    "\n",
    "print(f\"Number of spike events: {num_spike_events}\")\n",
    "print(f\"Spike frequency: {spike_frequency} Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b84ad3-4ecb-4057-a7e7-e88ade6795d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Troughs##\n",
    "# Find troughs between peaks\n",
    "troughs = []\n",
    "for i in range(len(peaks) - 1):\n",
    "    trough = np.argmin(depo_pA[peaks[i]:peaks[i+1]]) + peaks[i]\n",
    "    troughs.append(trough)\n",
    "troughs = np.array(troughs)\n",
    "\n",
    "# Plot the data with peaks and troughs\n",
    "plt.plot(time, depo_pA, label='100 pA')\n",
    "plt.plot(time[peaks], depo_pA[peaks], 'r.', label='Peaks')\n",
    "plt.plot(time[troughs], depo_pA[troughs], 'g.', label='Troughs')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('100 pA')\n",
    "plt.legend()\n",
    "plt.xlim(1,1.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0177d5-8a77-4556-9a67-bb103940c810",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(peaks) > 0:\n",
    "    # Identify the time of the last spike\n",
    "    last_spike_time = time[peaks[-1]]\n",
    "\n",
    "        # Calculate the time it took to go from onset to the last spike\n",
    "    onset_time = 1\n",
    "    time_to_last_spike = last_spike_time - onset_time\n",
    "    print(f\"Inactivation at: {time_to_last_spike} seconds\")\n",
    "else:\n",
    "    print(\"No spikes detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd02768a-cfe0-4fe5-9803-7534958ac27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the top-level folder path ('Data' folder)\n",
    "top_folder_path = 'C:/Users/koita/Downloads/FR-Am/FR/Data' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5067cc-adf2-46a8-8cc4-002142f18068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a20a05-1301-4ade-95fc-2e2de465e448",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b61bee-f8f9-4c5b-b390-fb559df66b23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c134d42f-7d62-41b4-ba25-0412e9fa9612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_data(top_folder_path):\n",
    "    # Initialize lists to store results\n",
    "    file_names = []\n",
    "    parent_folders = []  \n",
    "    subfolders = []  \n",
    "    num_spike_events_list = []\n",
    "    inter_spike_intervals_list = []\n",
    "    spike_frequency_list = []\n",
    "    time_to_last_spike_list = []\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    # Iterate through all subfolders within the top folder\n",
    "    for root, dirs, _ in os.walk(top_folder_path):\n",
    "        # Check if the current subfolder is a 'morphine' or 'NLX' folder\n",
    "        if os.path.basename(root) in ['Morphine', 'NLX']:\n",
    "            # Get the parent folder (Naive or MTA)\n",
    "            parent_folder = os.path.basename(os.path.dirname(root))\n",
    "            # Get the subfolder (morphine or NLX)\n",
    "            subfolder = os.path.basename(root)\n",
    "\n",
    "            # Iterate through all files in the current folder\n",
    "            for file_name in os.listdir(root):\n",
    "                if file_name.endswith('.axgt'):\n",
    "                    # Load data from the file\n",
    "                    file_path = os.path.join(root, file_name)\n",
    "                    data = np.loadtxt(file_path, skiprows=1, delimiter='\\t')\n",
    "\n",
    "                    # Extract time and data for analysis\n",
    "                    time = data[:, 0]\n",
    "                    experiment_data = data[:, 4]  # Change the column index as needed\n",
    "\n",
    "                    # Analysis results for: number of spikes/frequency/time to last spike  \n",
    "                    threshold = 0\n",
    "                    peaks, _ = find_peaks(experiment_data, height=threshold)\n",
    "\n",
    "                    num_spike_events = len(peaks)\n",
    "                    inter_spike_intervals = np.diff(time[peaks])\n",
    "\n",
    "                    if num_spike_events > 0:\n",
    "                        spike_frequency = num_spike_events / np.sum(inter_spike_intervals)\n",
    "                    else:\n",
    "                        spike_frequency = np.nan\n",
    "\n",
    "                    onset_time = 1\n",
    "                    time_to_last_spike = time[peaks[-1]] - onset_time if num_spike_events > 0 else None\n",
    "\n",
    "                    # Add results to lists\n",
    "                    file_names.append(file_name)\n",
    "                    parent_folders.append(parent_folder)  \n",
    "                    subfolders.append(subfolder)  \n",
    "                    num_spike_events_list.append(num_spike_events)\n",
    "                    inter_spike_intervals_list.append(inter_spike_intervals)\n",
    "                    spike_frequency_list.append(spike_frequency)\n",
    "                    time_to_last_spike_list.append(time_to_last_spike)\n",
    "\n",
    "                    count += 1\n",
    "\n",
    "    # Create a spreadsheet (CSV file) with the results\n",
    "    output_directory = os.path.join(top_folder_path, 'output')\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    output_file_path = os.path.join(output_directory, 'analysis_results.csv')\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'File Name': file_names,\n",
    "        'Parent Folder': parent_folders,  \n",
    "        'Subfolder': subfolders,  \n",
    "        'Number of Spike Events': num_spike_events_list,\n",
    "        'Inter-Spike Intervals': inter_spike_intervals_list,\n",
    "        'Spike Frequency (Hz)': spike_frequency_list,\n",
    "        'Time to Last Spike': time_to_last_spike_list\n",
    "    })\n",
    "\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"Results saved to {output_file_path}\")\n",
    "    print(f\"Number of times the loop ran: {count}\")\n",
    "\n",
    "# The top-level folder path ('Data' folder)\n",
    "top_folder_path = 'C:/Users/koita/Downloads/FR-Am/FR/Data'  \n",
    "analyze_data(top_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eeac09-86dd-402f-af02-15387e59f349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the path to the CSV file\n",
    "csv_path = 'C:/Users/koita/Downloads/FR-Am/FR/Data/output/analysis_results.csv'  \n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Create masks for each group\n",
    "naive_morphine_mask = (df['Parent Folder'] == 'Naive') & (df['Subfolder'] == 'morphine')\n",
    "naive_nlx_mask = (df['Parent Folder'] == 'Naive') & (df['Subfolder'] == 'NLX')\n",
    "mta_morphine_mask = (df['Parent Folder'] == 'MTA') & (df['Subfolder'] == 'morphine')\n",
    "mta_nlx_mask = (df['Parent Folder'] == 'MTA') & (df['Subfolder'] == 'NLX')\n",
    "\n",
    "# Apply masks to create separate DataFrames for each group\n",
    "naive_morphine_group = df[naive_morphine_mask]\n",
    "naive_nlx_group = df[naive_nlx_mask]\n",
    "mta_morphine_group = df[mta_morphine_mask]\n",
    "mta_nlx_group = df[mta_nlx_mask]\n",
    "\n",
    "# Display the sorted DataFrames\n",
    "print(\"Naive and Morphine Group:\")\n",
    "print(naive_morphine_group)\n",
    "print(\"\\nNaive and NLX Group:\")\n",
    "print(naive_nlx_group)\n",
    "print(\"\\nMTA and Morphine Group:\")\n",
    "print(mta_morphine_group)\n",
    "print(\"\\nMTA and NLX Group:\")\n",
    "print(mta_nlx_group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0475f22-53f4-4413-a87a-288c81975557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7bd6b5-70bb-4a9c-8c22-0c9dd9bcfa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# The path to the CSV file\n",
    "csv_path = 'C:/Users/koita/Downloads/FR-Am/FR/Data/output/analysis_results.csv'  # Update with your actual path\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Create masks for each group\n",
    "naive_morphine_mask = (df['Parent Folder'] == 'Naive') & (df['Subfolder'] == 'morphine')\n",
    "naive_nlx_mask = (df['Parent Folder'] == 'Naive') & (df['Subfolder'] == 'NLX')\n",
    "mta_morphine_mask = (df['Parent Folder'] == 'MTA') & (df['Subfolder'] == 'morphine')\n",
    "mta_nlx_mask = (df['Parent Folder'] == 'MTA') & (df['Subfolder'] == 'NLX')\n",
    "\n",
    "# Apply masks to create separate DataFrames for each group\n",
    "naive_morphine_group = df[naive_morphine_mask]\n",
    "naive_nlx_group = df[naive_nlx_mask]\n",
    "mta_morphine_group = df[mta_morphine_mask]\n",
    "mta_nlx_group = df[mta_nlx_mask]\n",
    "\n",
    "# Plot the average and individual points for each group\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot the individual points\n",
    "sns.stripplot(x='Parent Folder', y='Number of Spike Events', hue='Subfolder', data=df, jitter=True, dodge=True, alpha=0.9)\n",
    "\n",
    "# Plot the average for each group\n",
    "sns.barplot(x='Parent Folder', y='Number of Spike Events', hue='Subfolder', data=df, capsize=0.1)\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Experiment Group')\n",
    "plt.ylabel('Number of Spikes')\n",
    "plt.title('Average Number of Spikes in Each Group')\n",
    "\n",
    "# Show the plot\n",
    "plt.legend(title='Subfolder')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22de4ae-628f-464d-955a-f1cb635a8aae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Group by 'Parent Folder' and 'Subfolder'\n",
    "grouped_df = df.groupby(['Parent Folder', 'Subfolder'])\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Iterate through groups and plot average spike frequency\n",
    "for (parent_folder, subfolder), group_df in grouped_df:\n",
    "    \n",
    "    # Extract spike frequency from the group\n",
    "    spike_frequency = group_df['Spike Frequency (Hz)'].values\n",
    "\n",
    "    \n",
    "# Plot average spike frequency for each group\n",
    "average_frequency = grouped_df['Spike Frequency (Hz)'].mean()\n",
    "ax.bar(np.arange(len(average_frequency)), average_frequency, label='Average', color='black', alpha=0.65)\n",
    "\n",
    "# Set labels and legend\n",
    "ax.set_xticks(np.arange(len(average_frequency)))\n",
    "ax.set_xticklabels(average_frequency.index)\n",
    "ax.set_xlabel('Groups')\n",
    "ax.set_ylabel('Average Spike Frequency (Hz)')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6912aa2d-8e3a-44d6-90c9-60656aeaa43e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('Data/output/analysis_results.csv')\n",
    "\n",
    "# Assuming 'Inter-Spike Intervals' is a column in your DataFrame\n",
    "df['Inter-Spike Intervals'] = df['Inter-Spike Intervals'].apply(\n",
    "    lambda x: np.fromstring(x.strip('[]'), sep=' ').astype(float) if isinstance(x, str) else np.nan\n",
    ")\n",
    "\n",
    "# Group by 'Parent Folder' and 'Subfolder'\n",
    "grouped_df = df.groupby(['Parent Folder', 'Subfolder'])\n",
    "\n",
    "# Iterate through groups and calculate average of the first ISI\n",
    "for (parent_folder, subfolder), group_df in grouped_df:\n",
    "    first_isis = [isi[0] for isi in group_df['Inter-Spike Intervals'] if isinstance(isi, np.ndarray) and len(isi) > 0]\n",
    "    average_first_isi = np.mean(first_isis)\n",
    "    \n",
    "    print(f'{parent_folder} - {subfolder}: Average of the first ISI = {average_first_isi}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859c5455-0f4a-48c4-b295-d32ca3cfdd93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('Data/output/analysis_results.csv')\n",
    "\n",
    "# Assuming 'Inter-Spike Intervals' is a column in your DataFrame\n",
    "df['Inter-Spike Intervals'] = df['Inter-Spike Intervals'].apply(\n",
    "    lambda x: np.fromstring(x.strip('[]'), sep=' ').astype(float) if isinstance(x, str) else np.nan\n",
    ")\n",
    "\n",
    "# Group by 'Parent Folder' and 'Subfolder'\n",
    "grouped_df = df.groupby(['Parent Folder', 'Subfolder'])\n",
    "\n",
    "# Iterate through groups and calculate average of each ISI\n",
    "for (parent_folder, subfolder), group_df in grouped_df:\n",
    "    max_isi_length = max(len(isi) for isi in group_df['Inter-Spike Intervals'] if isinstance(isi, np.ndarray))\n",
    "    \n",
    "    for i in range(max_isi_length):\n",
    "        isis = [isi[i] for isi in group_df['Inter-Spike Intervals'] if isinstance(isi, np.ndarray) and len(isi) > i]\n",
    "        average_isi = np.mean(isis)\n",
    "        \n",
    "        print(f'{parent_folder} - {subfolder}: Average of ISI-{i+1} = {average_isi}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7090e08-ef14-41f7-b3cd-13f711c27a8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('Data/output/analysis_results.csv')\n",
    "\n",
    "# Assuming 'Inter-Spike Intervals' is a column in your DataFrame\n",
    "df['Inter-Spike Intervals'] = df['Inter-Spike Intervals'].apply(\n",
    "    lambda x: np.fromstring(x.strip('[]'), sep=' ').astype(float) if isinstance(x, str) else np.nan\n",
    ")\n",
    "\n",
    "# Group by 'Parent Folder' and 'Subfolder'\n",
    "grouped_df = df.groupby(['Parent Folder', 'Subfolder'])\n",
    "\n",
    "# Initialize a new DataFrame to store the averages\n",
    "averages_df = pd.DataFrame()\n",
    "\n",
    "# Iterate through groups and calculate average of each ISI\n",
    "for (parent_folder, subfolder), group_df in grouped_df:\n",
    "    max_isi_length = max(len(isi) for isi in group_df['Inter-Spike Intervals'] if isinstance(isi, np.ndarray))\n",
    "    \n",
    "    for i in range(max_isi_length):\n",
    "        isis = [isi[i] for isi in group_df['Inter-Spike Intervals'] if isinstance(isi, np.ndarray) and len(isi) > i]\n",
    "        average_isi = np.mean(isis)\n",
    "        \n",
    "        # Store the average ISI in the corresponding column\n",
    "        column_name = f'Group{i+1}'\n",
    "        averages_df.loc[f'{parent_folder} - {subfolder}', column_name] = average_isi\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(averages_df)\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "output_file_path = 'averages_output.csv'\n",
    "averages_df.to_csv(output_file_path)\n",
    "\n",
    "# Print the location of the output file\n",
    "print(f'Output file saved to: {output_file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3eb530-80af-41db-b1cb-5ec8317174b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('Data/output/analysis_results.csv')\n",
    "\n",
    "# Assuming 'Inter-Spike Intervals' is a column in your DataFrame\n",
    "df['Inter-Spike Intervals'] = df['Inter-Spike Intervals'].apply(\n",
    "    lambda x: np.fromstring(x.strip('[]'), sep=' ').astype(float) if isinstance(x, str) else np.nan\n",
    ")\n",
    "\n",
    "# Group by 'Parent Folder' and 'Subfolder'\n",
    "grouped_df = df.groupby(['Parent Folder', 'Subfolder'])\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Iterate through groups and plot the first three ISI\n",
    "for (parent_folder, subfolder), group_df in grouped_df:\n",
    "    max_isi_length = min(3, max(len(isi) for isi in group_df['Inter-Spike Intervals'] if isinstance(isi, np.ndarray)))\n",
    "    \n",
    "    for i in range(max_isi_length):\n",
    "        isis = [isi[i] for isi in group_df['Inter-Spike Intervals'] if isinstance(isi, np.ndarray) and len(isi) > i]\n",
    "        ax.plot(range(1, len(isis) + 1), isis, marker='o', label=f'{parent_folder} - {subfolder} - ISI-{i+1}')\n",
    "\n",
    "# Set labels and legend\n",
    "ax.set_xlabel('Experiment Index')\n",
    "ax.set_ylabel('Inter-Spike Interval')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa913d22-fb85-42e2-9ca6-1bd91ec3b243",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load your original DataFrame from the CSV file\n",
    "df = pd.read_csv('Data/output/analysis_results.csv')\n",
    "\n",
    "# Group by 'Parent Folder' and 'Subfolder'\n",
    "grouped_df = df.groupby(['Parent Folder', 'Subfolder'])\n",
    "\n",
    "# Define the maximum number of ISI indices\n",
    "max_isi_indices = 10  # You can adjust this based on your data\n",
    "\n",
    "# Create an empty DataFrame to store results\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "# Iterate through groups and calculate average ISI for each ISI index\n",
    "for (parent_folder, subfolder), group_df in grouped_df:\n",
    "    # Extract ISI values from the group\n",
    "    isi_values = [np.fromstring(isi.strip('[]'), sep=' ').astype(float) for isi in group_df['Inter-Spike Intervals'].values if isi]\n",
    "    \n",
    "    # Calculate average ISI for each ISI index\n",
    "    avg_isis = [np.nanmean([isi[i] for isi in isi_values if len(isi) > i]) for i in range(max_isi_indices)]\n",
    "    \n",
    "    # Create a label for the group\n",
    "    group_label = f'{parent_folder} - {subfolder}'\n",
    "\n",
    "    # Add the label as a new column in the results DataFrame\n",
    "    results_df[group_label] = avg_isis\n",
    "\n",
    "# Export the results DataFrame to a new CSV file\n",
    "results_df.to_csv('average_isis_results.csv', index=False)\n",
    "\n",
    "# Print a message indicating where the new file is saved\n",
    "print(\"Results saved to: average_isis_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce182007-5977-4b86-b6ae-2e491c932131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a NumPy array of floats\n",
    "isi_array = np.fromstring(isi_string.strip('[]'), sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c11035b-6311-4c31-996e-da73962b48fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find the minimum length between normalized_time and avg_isi\n",
    "min_length = min(len(normalized_time), len(avg_isi))\n",
    "\n",
    "# Plot average ISI\n",
    "ax.plot(normalized_time[:min_length], avg_isi[:min_length], label='Average', linewidth=2)\n",
    "\n",
    "# Set labels and legend\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Inter-Spike Interval')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc52326e-0ffa-4114-9547-3ac12208e7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average ISI\n",
    "average_isi = np.mean(np.vstack(isis), axis=0)\n",
    "ax.plot(normalized_time, average_isi, label=f'{parent_folder} - {subfolder}')\n",
    "\n",
    "max_length = max(len(isi) for isi in isis)\n",
    "padded_isis = [np.pad(isi, (0, max_length - len(isi)), constant_values=np.nan) for isi in isis]\n",
    "average_isi = np.nanmean(np.vstack(padded_isis), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe32d58-d4ad-4a32-9436-f2dc74d8c229",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e7f045-1db8-44db-ab53-5ddd0a768ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00a22d9-d5ae-48cb-bde5-63301d962657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b673dc-fdf0-438d-a3dd-dbd4853fd9dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eede42-b28f-494d-b8fe-9f91a2901c02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0b0c65-9c24-419b-90ae-e0f16a9dae39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a56caee-9566-4bfc-addc-9bb22c577f5f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "test1 = Test['c005_Analog_Input__0\\x00\\x00t\\x00']\n",
    "time = test['c001_Time\\x00\\x00\\x00PÃƒ\\x00\\x00']\n",
    "test1 = np.squeeze(test1)\n",
    "time = np.squeeze(time)\n",
    "\n",
    "\n",
    "start_index = 1\n",
    "end_index = 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1042f840-2cf2-4d23-95c9-d5b31b81e830",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time, Depo_pA)\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Depolarizing current injection (100pA)')\n",
    "#plt.xlim(start_index, end_index)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f85827-89c1-4fb4-b9c9-e05d0ac9f355",
   "metadata": {},
   "outputs": [],
   "source": [
    "##other shit## \n",
    "\n",
    "# Specify the time point you're interested in\n",
    "target_time = 0.8\n",
    "\n",
    "# Find the index in c001 corresponding to the target time\n",
    "index_c001 = np.argmin(np.abs(time_array_c001 - target_time))\n",
    "\n",
    "# Use the index to get the corresponding data point in c005\n",
    "data_point_c005 = data_array_c005[index_c001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1cfb23-7793-433f-9deb-487466b3e1a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd56d095-1ecf-45d0-ba37-9fbc3270eca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##average baseline##\n",
    "\n",
    "# Define the time points\n",
    "BL_start = 8000\n",
    "BL_end = 10000\n",
    "\n",
    "\n",
    "# Extract values between 0.8 seconds and 1 second  \n",
    "BL_v = test1[BL_start:BL_end]\n",
    "\n",
    "# Calculate the average of the baseline values\n",
    "BL = np.mean(BL_v)\n",
    "BL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec2610c-0a5d-402b-bfa7-fd8284c33994",
   "metadata": {},
   "outputs": [],
   "source": [
    "Zeroed_test= test1 - BL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c8397c-006e-4f51-a8c0-a5f034d75796",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time, Zeroed_test)\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Depolarizing current injection (100pA)')\n",
    "plt.xlim(start_index, end_index)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf33af6d-36fe-4c48-af49-1528c1b750dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the number of events/APs# \n",
    " #define range to compute\n",
    "   # define threshold \n",
    "    #ask how many times it goes over said threshold. \n",
    "    \n",
    "   # turn that into a frequency \n",
    "    \n",
    "#ASK when do these cells inactivate follwing depolatization??#   ***help \n",
    "    \n",
    "    \n",
    "#Find the ISI \n",
    " #plot the ISI from the initial AP\n",
    "    \n",
    "   # find the average refactory point following an AP\n",
    "    \n",
    "   # take that from a lot of different files and then group them. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc5046c-db32-4337-a178-ee0073f8423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install ipfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac4951f-f30f-44e0-ac22-48c97c5ecb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages \n",
    "from ipfx.feature_extractor import (SpikeFeatureExtractor,\n",
    "                                    SpikeTrainFeatureExtractor)\n",
    "import pyabf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878a92ac-309a-4f9f-8409-690324ac56b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "abf = pyabf.ABF(test1)\n",
    "\n",
    "print (abf)\n",
    "abf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcc4e00-4a87-4a6b-8bbc-c14e1a5f3772",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the table from the dataframe below\n",
    "dataframe = [] \n",
    " \n",
    "# Loop function to analyze each voltage trace of the file\n",
    "for sweepNumber in abf.sweepList:\n",
    "    abf.setSweep(sweepNumber)\n",
    "    time = abf.sweepX\n",
    "    voltage = abf.sweepY\n",
    "    current = abf.sweepC\n",
    "     \n",
    "    # Define the region (in seconds)\n",
    "    start, end = 0.24, 0.265\n",
    "     \n",
    "    # Parameters for analysis: \n",
    "    sfx = SpikeFeatureExtractor (start, end, \n",
    "                                 filter=None, # cutoff frequency for 4-pole low-pass Bessel filter in kHz \n",
    "                                 dv_cutoff=20.0,  # minimum dV/dt to qualify as a spike in V/s \n",
    "                                 thresh_frac=0.05,  # fraction of average upstroke for threshold calculation\n",
    "                                 min_peak=0)  # minimum acceptable absolute peak level in mV \n",
    "    sfx_results = sfx.process(time, voltage, current)\n",
    "    dataframe.append(sfx_results)  # To get the mean: df.append(sfx_results.mean())\n",
    "     \n",
    "# Table with the features from all the action potentials\n",
    "table = pd.concat(dataframe)\n",
    " \n",
    "# Optional: Plot the trace/s\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.xlabel (\"Time (s)\")\n",
    "plt.ylabel(\"Voltage (mV)\")\n",
    "for sweepNumber in abf.sweepList:  # Loop to plot all the traces\n",
    "    abf.setSweep(sweepNumber)\n",
    "    plt.plot(abf.sweepX, abf.sweepY, alpha=.6, label=\"sweep %d\" % (sweepNumber))\n",
    "# To highllight one trace\n",
    "abf.setSweep(3) \n",
    "plt.plot(abf.sweepX, abf.sweepY, linewidth=1, color='black')\n",
    "plt.xlim(0.24, 0.265)\n",
    "    \n",
    "# Display the graph and the table\n",
    "plt.show()\n",
    "table\n",
    " \n",
    "# Remove the below # to show only selected columns. E.g:\n",
    "# columns = ['threshold_i', 'threshold_v', 'width', 'upstroke', 'downstroke']\n",
    "# table[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de3fcdc-0885-40aa-974d-c8e1e46acd17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
