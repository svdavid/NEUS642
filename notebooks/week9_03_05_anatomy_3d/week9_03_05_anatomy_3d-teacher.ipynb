{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "<a href=\"#Overview\"></a>\n# Overview\n* <a href=\"#4c8dfb2f-2382-4144-93e7-e9da792a7034\">Week 9 - Imaging and anatomy</a>\n  * <a href=\"#f7c07563-1430-4288-bbdb-32224401f4d8\">Fun in Three Dimensions</a>\n  * <a href=\"#7cf740fc-f894-4484-a0f2-83ddeae677b3\">Introduction</a>\n  * <a href=\"#03520a2d-f1da-49ea-a66f-7168e52becad\">Loading the data, and exploring nibabel</a>\n    * <a href=\"#bbde4f95-8e34-4d83-a417-489125695d41\">So the volumes are both 'Nifti's' -- and it appears that they are 120 by 159 by 102 matrices -- that sounds like an 3D MRI volume to me!</a>\n      * <a href=\"#4b62402e-9483-4a3a-9e60-2f679d1cb6b8\">Let take a look at a single slice in the middle of the brain using imshow, a matplotlib version of \"plot\" for showing a 2D image</a>\n  * <a href=\"#e45c062d-1db5-471d-a701-1d460491e36d\">Learning About NIfTIs</a>\n* <a href=\"#c62bf9b4-8fc4-4348-a10e-9d5407062551\">NIfTI</a>\n  * <a href=\"#38e45e1c-da90-45b7-9879-8f55759d6642\">What is NIfTI?</a>\n  * <a href=\"#8a240e83-5b23-4c26-b63f-2190c1506c1c\">NIfTI file format</a>\n  * <a href=\"#04ef3b1d-19cd-43f7-885e-f358b49805aa\">DICOM vs NlfTI</a>\n    * <a href=\"#6189a6fd-db18-4063-a7f1-d444e51a5263\">But we are not going to bother with DICOMS (today)</a>\n  * <a href=\"#aac4ed9e-b87e-4674-857b-af273894a05f\">Back to our Nibabel Nifti object --</a>\n* <a href=\"#5cc1947a-569a-4c15-b2ad-3d2dd1766bf4\">Let's pull these out!</a>\n  * <a href=\"#3cc01d52-0ff1-4dcc-9d3d-e8af64d04cdc\">Exercise 1: fix overlaid segmentation</a>\n  * <a href=\"#a4edc731-b09e-4381-a58b-9cf99ca7668e\">Excercise 2 - Excise Neocortex</a>\n  * <a href=\"#2b9e9b8f-528a-47fa-9769-4f5c4200549a\">Excercise 3 - Create individual tissue segmentations</a>\n  * <a href=\"#d0bc3d65-621c-451a-869c-11d1d9721716\">Excercise 4 -- Euclidean Distance Transforms</a>\n* <a href=\"#cdc66cf9-ba23-4038-bea0-330474d728a9\">Huzzah!</a>\n  * <a href=\"#9e24bccb-5f35-4341-863e-c3f12a4c0377\">Now we have what we have always dreamed of, a white matter mask that has information regarding its distance to cortex.</a>\n  * <a href=\"#774eeef7-82cb-491c-9e8b-ab716a57be8b\">Excercise-ish - Make plotting pretty/easier.</a>\n  * <a href=\"#a23a9851-f4ae-49cd-b10c-98fc97ebf65c\">Extra Excercise - Mid-cortical Thickness</a>"}, {"cell_type": "markdown", "id": "93e61e2d-0dc1-40fb-9ce0-70ad5ef86f97", "metadata": {"tags": []}, "source": ["<a id=\"4c8dfb2f-2382-4144-93e7-e9da792a7034\"></a>\n# Week 9 - Imaging and anatomy\n<a href=\"#Overview\">Return to overview</a>\n", "\n", "<a id=\"f7c07563-1430-4288-bbdb-32224401f4d8\"></a>\n## Fun in Three Dimensions\n<a href=\"#Overview\">Return to overview</a>\n"]}, {"cell_type": "markdown", "id": "f40ebdc4-1822-45dc-b514-28ddc6807e58", "metadata": {}, "source": ["<a id=\"7cf740fc-f894-4484-a0f2-83ddeae677b3\"></a>\n## Introduction\n<a href=\"#Overview\">Return to overview</a>\n", "\n", "Today we're going to play around with some imaging data, and use some masking techniques to analyze tissue white matter intensity in a more anitomcally relevant way -- as a function of the distance from the surrounding grey matter. Often, groups will average large regions of white matter in analyses which neglects potentially significant spatial differences in organization. I am particularly interested in fetal and early infant brain development, and we can see that the spatial white matter characteristics vary across development. \n", "\n", "<td><img src=\"wm_devel.jpg\" /></td>\n", "\n", "<td><img src=\"gyri.jpg\" /></td>\n", "\n", "In addition to the specifics of dealing with imaging data/3 dimensional matrices, this will be an opportunity to become familiar with some anatomy and do a bit of light linear algebra.\n", "\n", "Main objectives\n", "\n", "1. Reading data from NIfTI-format files\n", "    - This will serve you well in when working with any imaging modality: MR, CT, US, etc.  \n", "\n", "2. Showing 2D slices (images) and creating a scrollable GUI\n", "    - How do we view a 3D volume without rendering? (Slices!)\n", "\n", "3. Masking out specific regions of interest the anatomical image for focused analysis. \n", "    - This requires aligning the raw data with known anatomical landmarks, often accomplished with registrations to labeled\n", "      atlases. \n", "    - Masking allows us to single out tissues, and make powerful calculations important for image processing.\n", "\n", "4. Measuring 3 dimensional distances.\n", "    - A simple Euclidean Distance Transform can be an incredibly useful tool for image analysis.  \n", "\n", "5. Some plot prettifying techniques\n", "    - Python is a really great way to consistently plot data in visually appealing ways. We will take advantage of this!\n", "\n", "This software runs directly from your python interpreter, but the first thing we'll need to do is install some packages.\n", "\n", "If you haven't already installed nibabel and pyqt, do it now:\n", "\n", "    conda install nibabel pyqt\n", "    \n"]}, {"cell_type": "code", "execution_count": null, "id": "88da4f71-15a0-4df1-a98c-c1244a9afd4c", "metadata": {"tags": []}, "outputs": [], "source": ["#First we will nee to import some libraries\n", "import os\n", "import scipy\n", "import numpy as np\n", "import nibabel as nib\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "markdown", "id": "1b1ef94d-d238-49d9-8ff9-ce46a3c361ca", "metadata": {"tags": []}, "source": ["<a id=\"03520a2d-f1da-49ea-a66f-7168e52becad\"></a>\n## Loading the data, and exploring nibabel\n<a href=\"#Overview\">Return to overview</a>\n", "\n", "The brain volume and corresponding tissue segmentation are stored in the directory \"data\"\n", "Lets make path variables for them "]}, {"cell_type": "code", "execution_count": null, "id": "4ea8b71b-28df-459b-bff8-33fd7ef9e647", "metadata": {"tags": []}, "outputs": [], "source": ["T2brainpath = os.path.join('data', 'ONPRC18_T2W_brain.nii.gz')\n", "orig_segpath = os.path.join('data', 'growth_label_onprc.nii.gz')"]}, {"cell_type": "markdown", "id": "acc633a8-ac5c-4736-be2b-70857f889bf5", "metadata": {"tags": []}, "source": ["Now we are going to load them into python with the help of nibabel -- do you see anything that might look promising?"]}, {"cell_type": "code", "execution_count": null, "id": "6d99d441-33a8-4bd8-ba3f-855f002bbfb9", "metadata": {"tags": []}, "outputs": [], "source": ["nib?\n"]}, {"cell_type": "code", "execution_count": null, "id": "7e6d1fa8-3966-4820-89ce-8699c0570e61", "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "id": "8b51240e-ae65-439c-b418-95c8751ec91a", "metadata": {}, "source": ["What helpful documentation! Load in the brain volume and corresponding segmentation, and print() the brain!"]}, {"cell_type": "code", "execution_count": null, "id": "fdcf47a5-2f6c-422a-beec-b01346f95ff7", "metadata": {"tags": []}, "outputs": [], "source": ["img1 = nib.load(T2brainpath)\n", "img2 = nib.load(orig_segpath)\n", "print(img1)"]}, {"cell_type": "markdown", "id": "eadd999e-459e-4596-85a5-ffd12d497d81", "metadata": {}, "source": ["Interesting... what the heck did we just load in?"]}, {"cell_type": "code", "execution_count": null, "id": "0533c7fd-d43f-4405-afc7-47b46e5a7e03", "metadata": {"tags": []}, "outputs": [], "source": ["print(type(img1))\n", "print(type(img2))\n", "print(img1.shape)\n", "print(img2.shape)"]}, {"cell_type": "markdown", "id": "858100e8-4fe9-43d2-aaee-0dfce019b867", "metadata": {}, "source": ["<a id=\"bbde4f95-8e34-4d83-a417-489125695d41\"></a>\n### So the volumes are both 'Nifti's' -- and it appears that they are 120 by 159 by 102 matrices -- that sounds like an 3D MRI volume to me!\n<a href=\"#Overview\">Return to overview</a>\n"]}, {"cell_type": "markdown", "id": "dade84ec-c16f-4e71-9c94-79d16ced5bfd", "metadata": {}, "source": ["So they are both 3D matrices of the same shape (120 by 159 by 102) units, or 'voxels' \n", "If the voxel size is 0.5mm isotropic (or 0.5mm on all sides) how big is our volume (in this case the FOV for the image) in real world units?"]}, {"cell_type": "code", "execution_count": null, "id": "53f9be32-f53f-4172-9549-33b0ea282b9a", "metadata": {"tags": []}, "outputs": [], "source": ["120*159*102*0.5/1000\n", "#So its about a liter! That seems reasonable "]}, {"cell_type": "markdown", "id": "ed50113b-6e9c-42ca-88c0-9587fabe0c6b", "metadata": {}, "source": ["<a id=\"4b62402e-9483-4a3a-9e60-2f679d1cb6b8\"></a>\n#### Let take a look at a single slice in the middle of the brain using imshow, a matplotlib version of \"plot\" for showing a 2D image\n<a href=\"#Overview\">Return to overview</a>\n"]}, {"cell_type": "code", "execution_count": null, "id": "28a3f0ac-d41f-48c8-9187-20f5acd2024c", "metadata": {"tags": []}, "outputs": [], "source": ["plt.imshow(img1[:,:,50],cmap='gray')\n"]}, {"cell_type": "markdown", "id": "7027772c-26b8-484b-8ec4-b5286e9cc453", "metadata": {}, "source": ["Uh oh, it looks like that wont work, but we find a helpful hint for how to actually do this!\n", "But first, lets figure out what these niftis atually are"]}, {"cell_type": "markdown", "id": "0e7238bf", "metadata": {}, "source": ["<a id=\"e45c062d-1db5-471d-a701-1d460491e36d\"></a>\n## Learning About NIfTIs\n<a href=\"#Overview\">Return to overview</a>\n"]}, {"cell_type": "markdown", "id": "1e6319cd-56bd-41be-9aff-eb1cf2e8a38e", "metadata": {}, "source": ["\n", "\n", "This information is adapted from https://www.tasq.ai/glossary/nifti/ and https://nipy.org/nibabel/coordinate_systems.html\n", "\n", "<a id=\"c62bf9b4-8fc4-4348-a10e-9d5407062551\"></a>\n# NIfTI\n<a href=\"#Overview\">Return to overview</a>\n", "<a id=\"38e45e1c-da90-45b7-9879-8f55759d6642\"></a>\n## What is NIfTI?\n<a href=\"#Overview\">Return to overview</a>\n", "MRI Data (and other imaging modalities like CT, etc.), may be saved in a special file format called NIfTI (Neuroimaging Informatics Technology Initiative). The discipline of medical imaging, as well as brain research and other image-based medical specialties including radiation oncology and psychiatry, make extensive use of it.\n", "\n", "The NIfTI format was created to improve upon the shortcomings of the previous standard for storing medical imaging data, the Analyze format.\n", "Along with the actual picture data, NIfTI files also include metadata such as the image\u2019s measurements, voxel size, and direction, as well as the results of any pre-processing that may have been performed on the raw data.\n", "\n", "For example, common viewers are all capable of reading NIfTI files (which end in .nii or .nii.gz).\n", "\n", "<a id=\"8a240e83-5b23-4c26-b63f-2190c1506c1c\"></a>\n## NIfTI file format\n<a href=\"#Overview\">Return to overview</a>\n", "There are two components to a NIfTI file:\n", "\n", "* the actual image data \n", "* header information \n", "\n", "_Both the picture data and the header information are kept as binary and text files, respectively._\n", "\n", "Some of the fields that make up a NIfTI header format are as follows:\n", "\n", "* Dimensions\u2013 The number of slices in the z-dimension is an example of a dimension. Dimensions are the total number of voxels in the picture.\n", "* Resolution\u2013 Millimeter values for x, y, and z dimensions define the spatial resolution of a 3D model.\n", "* Data- The image\u2019s data type, which might be an unsigned 8-bit integer or a 32-bit floating point number.\n", "* Orientation\u2013 Picture orientation refers to the overall orientation of an image, including the x, y, and z axes.\n", "* Intensity\u2013 If the data has been scaled in any manner, this is the scale factor for the intensity values.\n", "* Time\u2013 If the picture is a time series, the time step is the interval between frames.\n", "* History\u2013 An account of the data\u2019s past processing, including any smoothing or normalization that may have occurred.\n", "* Data offset\u2013 indicates the position in bytes from which the picture data begins to be read.\n", "* Extra fields\u2013 These are fields used to store data unique to particular picture formats, such as those used in diffusion-weighted images, or to record data that is unique to certain applications.\n", "\n", "<a id=\"04ef3b1d-19cd-43f7-885e-f358b49805aa\"></a>\n## DICOM vs NlfTI\n<a href=\"#Overview\">Return to overview</a>\n", "File formats for medical imaging data such as DICOM and NIfTI are similar yet serve distinct functions. NIfTI is mostly employed in the fields of medical imaging and neuroscience research, whereas DICOM is the standard file format for storing and distributing medical pictures.\n", "\n", "There are numerous medical imaging software programs and analytic tools that only work with NIfTI files, therefore converting DICOM files is a regular chore. It is possible to transform DICOM to NIfTI using a number of tools and packages, such as:\n", "\n", "        pydicom\u2013 Python\u2019s pydicom package may be used to read and write DICOM files and convert them to NIfTI.\n", "        \n", "<a id=\"6189a6fd-db18-4063-a7f1-d444e51a5263\"></a>\n### But we are not going to bother with DICOMS (today)\n<a href=\"#Overview\">Return to overview</a>\n", "\n", "<a id=\"aac4ed9e-b87e-4674-857b-af273894a05f\"></a>\n## Back to our Nibabel Nifti object --\n<a href=\"#Overview\">Return to overview</a>\n", "_A nibabel (and nipy) image is the association of three things:_\n", "\n", "* The image data array: a 3D or 4D array of image data\n", "\n", "* An affine array that tells you the position of the image array data in a reference space.\n", "\n", "* image metadata (data about the data) describing the image, usually in the form of an image header.\n", "\n", "So we know that our nifti's are not just the data, they are a combination of header and image data, as well as an associated affine component\n", "<a id=\"5cc1947a-569a-4c15-b2ad-3d2dd1766bf4\"></a>\n# Let's pull these out!\n<a href=\"#Overview\">Return to overview</a>\n"]}, {"cell_type": "code", "execution_count": null, "id": "418a27d6-c037-4f0b-a477-58cb47838401", "metadata": {"tags": []}, "outputs": [], "source": ["data = img1.get_fdata()\n", "header = img1.header\n", "afftrans = img1.affine \n", "print(data)\n", "print(header)\n", "print(afftrans)"]}, {"cell_type": "markdown", "id": "4e5fd2d6", "metadata": {}, "source": ["A problem I often run into is that the preview of a brain volume, ususally a very large matrix with surronding 0's, is not helpful.\n", "Sometimes, I just want a big ol' ugly print out of a bunch of numbers (a slice of the image). We can use _with_ to accomplish this for a unique instance by only momentarily changing some numpy print option parameters."]}, {"cell_type": "code", "execution_count": null, "id": "cbf12176", "metadata": {}, "outputs": [], "source": ["with np.printoptions(threshold=np.inf):\n", "    print(data[:,:,50])"]}, {"cell_type": "markdown", "id": "cf512586-4574-4385-9ff7-1ce85848787f", "metadata": {}, "source": ["Now, try to show a slice again, this time referencing the data object"]}, {"cell_type": "code", "execution_count": null, "id": "3ce14cec-9f59-4bf9-befa-e7929026e366", "metadata": {"tags": []}, "outputs": [], "source": ["plt.imshow(data[:,:,50],cmap='gray')"]}, {"cell_type": "markdown", "id": "7ae6a791-3231-48e5-a548-5cfd031d17a0", "metadata": {}, "source": ["Great, now overlay the same slice with the corresponding slice of the segmentation, using the colormap 'tab20'"]}, {"cell_type": "code", "execution_count": null, "id": "f3b4215b-0955-4c0a-b344-41a9769829ab", "metadata": {}, "outputs": [], "source": ["T1_vol=img1.get_fdata().copy()\n", "segmented_vol=img2.get_fdata().copy()"]}, {"cell_type": "markdown", "id": "acea997f-b598-434a-b581-b2714326cda4", "metadata": {}, "source": ["Here's what the segmenation looks like on its own. Notice the effect of changing the colormap"]}, {"cell_type": "code", "execution_count": null, "id": "875c868f-0583-46b5-b457-21837ca374c0", "metadata": {"tags": []}, "outputs": [], "source": ["fig, ax = plt.subplots(1,2, figsize=(12,6))\n", "ax[0].imshow(segmented_vol[:,:,50],cmap='gray')\n", "ax[1].imshow(segmented_vol[:,:,50], alpha = 1, cmap ='tab20')"]}, {"cell_type": "markdown", "id": "47ea3271-60b9-416e-9a1e-71a1ff5df729", "metadata": {}, "source": ["We can use the `alpha` parameter to superimpose a "]}, {"cell_type": "code", "execution_count": null, "id": "88456c04-9321-4c43-804f-d9b05d63b1b2", "metadata": {"tags": []}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(12,8))\n", "ax.imshow(T1_vol[:,:,50],cmap='gray')\n", "ax.imshow(segmented_vol[:,:,50], alpha = .1, cmap ='tab20')"]}, {"cell_type": "markdown", "id": "048a2a92-ff8f-43b6-b28a-afc9c6f3dc07", "metadata": {}, "source": ["<a id=\"3cc01d52-0ff1-4dcc-9d3d-e8af64d04cdc\"></a>\n## Exercise 1: fix overlaid segmentation\n<a href=\"#Overview\">Return to overview</a>\n", "\n", "The values for the segmentation are not well distributed, with subcortical grey matter values arbitrarily very high\n", "Create a new segmentation `segmented_vol2` with all values higher than 10 set to 10, and overlay it. Hint: start by setting `segmented_vol2 = segmented_vol` and then thresholding"]}, {"cell_type": "code", "execution_count": null, "id": "ef70f621-c2d4-4345-8b2e-c0cf5d777408", "metadata": {"tags": []}, "outputs": [], "source": ["segmented_vol2=img2.get_fdata().copy()\n", "%load \"answers/answer_001.txt\""]}, {"cell_type": "markdown", "id": "615ca544-987d-4a79-a608-deb2d9c18fca", "metadata": {"tags": []}, "source": ["Nice! What a clean(ish) segementation! \n", "There is not an easy way (that I know of) to view 3D images as a series of 2D slices, but the below code will let us see the full 3D volume"]}, {"cell_type": "code", "execution_count": null, "id": "a7cf6484-021c-408f-8546-8219499a3c54", "metadata": {"tags": []}, "outputs": [], "source": ["class IndexTracker(object):\n", "    def __init__(self, ax, X):\n", "        self.ax = ax\n", "        ax.set_title('use scroll wheel to navigate images')\n", "\n", "        self.X = X\n", "        rows, cols, self.slices = X.shape\n", "        self.ind = self.slices//2\n", "\n", "        self.im = ax.imshow(self.X[:, :, self.ind])\n", "        self.update()\n", "\n", "    def onscroll(self, event):\n", "        print(\"%s %s\" % (event.button, event.step))\n", "        if event.button == 'up':\n", "            self.ind = (self.ind + 1) % self.slices\n", "        else:\n", "            self.ind = (self.ind - 1) % self.slices\n", "        self.update()\n", "\n", "    def update(self):\n", "        self.im.set_data(self.X[:, :, self.ind])\n", "        self.ax.set_ylabel('slice %s' % self.ind)\n", "        self.im.axes.figure.canvas.draw()\n", "\n", "%matplotlib qt \n", "fig, ax = plt.subplots(1, 1)\n", "\n", "tracker = IndexTracker(ax, data)\n", "\n", "\n", "fig.canvas.mpl_connect('scroll_event', tracker.onscroll)\n", "plt.show()"]}, {"cell_type": "markdown", "id": "3b42c7e5-c303-4f88-b8af-9a92a8f6829b", "metadata": {}, "source": ["<a id=\"a4edc731-b09e-4381-a58b-9cf99ca7668e\"></a>\n## Excercise 2 - Excise Neocortex\n<a href=\"#Overview\">Return to overview</a>\n", "What a groovy looking brain. Ok, Now that we have taken a a good look at our data, lets actually do some business. \n", "We are interested in neocortical grey and white matter, so the brainstem and cerebellum are going to be a problem. \n", "Create a new segmentation called neoseg, that includes everything except the cerebellum and brainstem, which are coded as 7s and 8s in the original segmentation."]}, {"cell_type": "code", "execution_count": null, "id": "33ab24d4-fe54-42d5-b347-f79ca3765e35", "metadata": {}, "outputs": [], "source": ["neoseg = img2.get_fdata().copy()\n", "%load \"answers/answer_002.txt\""]}, {"cell_type": "markdown", "id": "4625391f-a955-4409-a870-f9195f9cf3e4", "metadata": {}, "source": ["<a id=\"2b9e9b8f-528a-47fa-9769-4f5c4200549a\"></a>\n## Excercise 3 - Create individual tissue segmentations\n<a href=\"#Overview\">Return to overview</a>\n", "Stellar work, yall. \n", "So, now what we want is to create a new map of the white matter tissue for the whole brain, where each voxel's value is the distance to the nearest non-white matter voxel. \n", "Easy -- to do this we only need 3ish things.\n", " First, initialize temporary volumes filled with zeros the same size as out brain/segmentation volumes, and name them gm and wm and csf (for later). \n", " _Hint: newvol=volofchoice*0_"]}, {"cell_type": "code", "execution_count": null, "id": "f456b08e-e713-4d9c-842c-d7ce9b930fc4", "metadata": {"tags": []}, "outputs": [], "source": ["%load \"answers/answer_003.txt\""]}, {"cell_type": "markdown", "id": "fd61d562-2f8f-47d2-8602-43011ea5bac1", "metadata": {}, "source": ["Great, now put 1s in each matrix corresponding to the original segmentation for each of these new volumes -- HINT! In the original segmentation, or your new segpar variable, CSF corresponds to 1, GM corresponds to 2, and WM corresponds to 3!\n"]}, {"cell_type": "code", "execution_count": null, "id": "e12d9a65-6844-493b-9c4f-ad202effe826", "metadata": {"tags": []}, "outputs": [], "source": ["%load \"answers/answer_004.txt\""]}, {"cell_type": "markdown", "id": "d36a4c79-4864-4e50-8b77-af6ab384fd38", "metadata": {}, "source": ["<a id=\"d0bc3d65-621c-451a-869c-11d1d9721716\"></a>\n## Excercise 4 -- Euclidean Distance Transforms\n<a href=\"#Overview\">Return to overview</a>\n", "Now that we have these volumes, we can get tricky with calculating distances. \n", "Remember, we want white matter to not be a binary \"1\" denoting yes this tissue is in fact white matter, but instead denote its location in a bank of white matter, in other words, that white matter voxels distance from the closest non-white matter voxel.\n"]}, {"cell_type": "markdown", "id": "e33dc506-fcb3-486c-9481-ad00437a94cc", "metadata": {}, "source": ["Thankfully scipy has a function _ndimage.distance_transform_edt_ that allows one to caclulate the exact Euclidean distance transform, which calculates the distance transform of the input, by replacing each foreground (non-zero) element, with its shortest distance to the background (any zero-valued element).\n", "\n", "Suppose we have the binary image below, what does the distance transform look like? Try for yourself and label it bindist -- Hint: the function takes the argument 'f(array==value)'"]}, {"cell_type": "code", "execution_count": null, "id": "011dede6-b7da-4c9e-8992-32bef46e76a3", "metadata": {"tags": []}, "outputs": [], "source": ["binary_image = np.array([[0,0,1,0,0],\n", "                         [0,1,1,1,0],\n", "                         [0,1,1,1,0],\n", "                         [0,0,1,0,0],])"]}, {"cell_type": "code", "execution_count": null, "id": "5623b620-8a6b-4f68-bbbd-c3044d56afe6", "metadata": {"tags": []}, "outputs": [], "source": ["%load \"answers/answer_005.txt\""]}, {"cell_type": "markdown", "id": "338da991-c9e6-4f71-a787-b02ed202fe62", "metadata": {}, "source": ["Awesome, does that look like you would expect?\n"]}, {"cell_type": "markdown", "id": "55df31f0-ffd8-4fae-b766-8ed011008e23", "metadata": {}, "source": ["Now we can do the same thing but for our binary tissue maps!!\n", "Create a new volume called wmdist that represents very voxels distance from the grey matter, then SHOW THAT PUPPY."]}, {"cell_type": "code", "execution_count": null, "id": "936655a3-cf42-455c-88e5-5ebf9599575b", "metadata": {"tags": []}, "outputs": [], "source": ["%load \"answers/answer_006.txt\""]}, {"cell_type": "code", "execution_count": null, "id": "df4fc630", "metadata": {}, "outputs": [], "source": ["#To help with showing\n", "fig, ax = plt.subplots(figsize=(12,8))\n", "ax.imshow(wmdist[:,:,50],cmap='gray')"]}, {"cell_type": "markdown", "id": "2ace9b83-208c-4e91-b473-3aff8ec7caf1", "metadata": {}, "source": ["Heck yeah! So, now we have every non-GM voxel's distance from GM. \n", "But, I think we should save the vignette for instagram... I dont think we care very much about knowing how far away we are from cortex in CSF. \n", "\n", "We can get rid of all that by masking out all the voxels that arent white matter to zero. \n", "\n", "_Any thoughts?_"]}, {"cell_type": "code", "execution_count": null, "id": "711a93b5-881c-4438-be03-d8ae3ed00783", "metadata": {"tags": []}, "outputs": [], "source": ["%load \"answers/answer_007.txt\""]}, {"cell_type": "code", "execution_count": null, "id": "e963734e", "metadata": {}, "outputs": [], "source": ["#To help with showing\n", "fig, ax = plt.subplots(figsize=(12,8))\n", "ax.imshow(wmdist[:,:,50],cmap='gray')"]}, {"cell_type": "markdown", "id": "7d3b8154-2483-4772-a644-39292ff116a2", "metadata": {}, "source": ["<a id=\"cdc66cf9-ba23-4038-bea0-330474d728a9\"></a>\n# Huzzah!\n<a href=\"#Overview\">Return to overview</a>\n", "<a id=\"9e24bccb-5f35-4341-863e-c3f12a4c0377\"></a>\n## Now we have what we have always dreamed of, a white matter mask that has information regarding its distance to cortex.\n<a href=\"#Overview\">Return to overview</a>\n"]}, {"cell_type": "markdown", "id": "3c63cb01", "metadata": {}, "source": ["Lets zoom into the left inferior temporal gyrus (ITG) to get a closer look at what we just accomplished.  "]}, {"cell_type": "code", "execution_count": null, "id": "29c9e4f9", "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(12,8))\n", "left_ITG=data[95:120,75:110,50]\n", "left_ITG_dist=wmdist[95:120,75:110,50]\n", "ax.imshow(left_ITG,cmap='gray')\n", "ax.imshow(left_ITG_dist,cmap='viridis', alpha = .2)"]}, {"cell_type": "markdown", "id": "c32e334b-52f5-49bb-8ea8-b094f8c8eb97", "metadata": {}, "source": ["So now we just need to get an account of every one of these voxel's \"distance values\" and the corresponding image intensity values, and then mean the intensity values by unique distance value. "]}, {"cell_type": "code", "execution_count": null, "id": "55034854-1538-41eb-9e64-0588ceb156d5", "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "id": "84116502-0755-48bc-83e2-86032cff9e84", "metadata": {"tags": []}, "outputs": [], "source": ["#Let's get all distances ignoring 0's\n", "distances = wmdist[wmdist > 0]\n", "#y then gets all those overlapping image voxels that arent at 0 distance\n", "intensities = T1_vol[wmdist > 0]\n", "\n", "#using np.unique, we can get all the unique values of our array,\n", "#while also getting a  also return the indices of the unique array\n", "#that can be used to reconstruct our array -- this is needed to be able to\n", "# average all of the intensity values subsequently\n", "distances_new,inv,cnt = np.unique(distances,return_inverse=True,return_counts=True)\n", "# now we feed bincount the indices from above (inv, and give our intensity values\n", "# as the weights, and divide by the count for each unique index!!\n", "\n", "intensities_new = np.bincount(inv,weights=intensities)/cnt\n", "#Here is another way to accomplish the same thing uising an ordered dictionary\n", "#from collections import OrderedDict\n", "#values = OrderedDict()\n", "#for v1, v2 in zip(x, y):\n", "#    values.setdefault(v1, []).append(v2)\n", "\n", "#new_x = list(values.keys())\n", "#new_y = [sum(values[i]) / len(values[i]) for i in values]\n", "\n"]}, {"cell_type": "markdown", "id": "3d97f866-6bb8-4d63-826d-48c9e1281b3e", "metadata": {}, "source": ["Alright team, nice work -- now how many voxels are we querrying?\n", "Once you figure that out, plot the distances_new and intensities_new out to 5mm!!"]}, {"cell_type": "code", "execution_count": null, "id": "90d129a6-6bf5-4b08-89f6-a281c4922f94", "metadata": {"tags": []}, "outputs": [], "source": ["%load \"answers/answer_008.txt\""]}, {"cell_type": "markdown", "id": "46a68b52-03d7-4088-9d30-33b7dff622c4", "metadata": {}, "source": [" Neato, we can see that intially, white matter gets darker to a minimum a little more than one mm away from cortex. This corresponds to the center of your average fasicle. However, we then see an increase in intensity at greater differences. \n", "Why do you think this is? (Think about the internal capsule, imperfect segmentations, and the ridiculousness of trying to average every WM voxel across the brain! ...But we had fun"]}, {"cell_type": "markdown", "id": "b15a0d7f-4fbc-4ddb-a17c-45eeb84cdbd9", "metadata": {}, "source": ["<a id=\"774eeef7-82cb-491c-9e8b-ab716a57be8b\"></a>\n## Excercise-ish - Make plotting pretty/easier.\n<a href=\"#Overview\">Return to overview</a>\n", "That is one ugly plot -- lets have fun making it pretty. Starting with RC params!\n", "Customizing Matplotlib with style sheets and rcParams\n", "Tips for customizing the properties and default styles of Matplotlib.\n", "\n", "There are three ways to customize Matplotlib:\n", "\n", "Setting rcParams at runtime.\n", "\n", "Using style sheets.\n", "\n", "Changing your matplotlibrc file.\n", "\n", "Setting rcParams at runtime takes precedence over style sheets, style sheets take precedence over matplotlibrc files."]}, {"cell_type": "code", "execution_count": null, "id": "345afb99-5bf4-45c9-9839-6884cb47554a", "metadata": {"tags": []}, "outputs": [], "source": ["import matplotlib as mpl\n", "\n", "SMALL_SIZE = 30\n", "MEDIUM_SIZE = 30\n", "BIGGER_SIZE = 40\n", "\n", "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n", "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n", "plt.rc('axes', labelsize=SMALL_SIZE)    # fontsize of the x and y labels\n", "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n", "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n", "plt.rc('legend', fontsize=14)    # legend fontsize\n", "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n", "mpl.rcParams['font.family'] = 'Arial'"]}, {"cell_type": "code", "execution_count": null, "id": "2049b2e9-abd2-4bee-af37-3caf3112af18", "metadata": {"tags": []}, "outputs": [], "source": ["from matplotlib.lines import Line2D\n", "import matplotlib.patches as mpatches\n", "\n", "#Intitialize Figure\n", "fig, ax = plt.subplots(figsize=(12,8))\n", "#Make a fun arbitrary conditional color scheme\n", "wackycolors = np.where(distances_new<3,'peachpuff',np.where(intensities_new<2.4,'lightseagreen','rebeccapurple'))\n", "#Plot that puppy\n", "plt.scatter(distances_new, intensities_new, marker = '^', color=wackycolors)\n", "#Set the x limit to 10\n", "plt.xlim(0,10)\n", "#Unneccesarily rotate the x tick labels\n", "plt.xticks(rotation = 45, ha = 'right', rotation_mode = 'anchor')\n", "#Title plot as a compliment, and move it to the inside of your plot, and shift to the left a bit to leave room for the legend\n", "plt.title(\"Cool Graph Bro\", x=0.40, y = 1, pad = -28)\n", "#Axis labels\n", "plt.ylabel(\"T2 Intensity (Arbitrary)\")\n", "plt.xlabel(\"Distance from Cortex (mm)\" )\n", "#Create your custom legend, because we can!\n", "#Use mpatches for bar type plots, and Line2D for line/scatter plots\n", "custom_lines = [mpatches.Patch(facecolor='pink', edgecolor='k'),\n", "                Line2D([0], [0], marker = '^', color=\"black\", lw=0,markersize=15),\n", "               Line2D([0], [0], color='red', linewidth=1.5)]\n", "plt.legend(custom_lines, [\"Pink Patch\",\"Black Triangle Point\",'Red$_{Line}$' ],fontsize=20)"]}, {"cell_type": "markdown", "id": "fc005ae3", "metadata": {}, "source": ["<a id=\"a23a9851-f4ae-49cd-b10c-98fc97ebf65c\"></a>\n## Extra Excercise - Mid-cortical Thickness\n<a href=\"#Overview\">Return to overview</a>\n"]}, {"cell_type": "markdown", "id": "59f51486-443d-4e01-a1fa-265eb8435095", "metadata": {}, "source": ["OK, if we have time we will make a mid cortical thickness mask which is super useful for any kind of surface based analysis. "]}, {"cell_type": "markdown", "id": "2d3c3a56-4b7a-406f-8a0c-13c7453564f5", "metadata": {}, "source": ["Now the thinky bit -- we want to get the distance for outside voxels into the brain and only wm voxels out of the brain... how do we do this?\n", "We need 2 volumes, 'inside' and 'outside' with 1s for tissue in the brain (inside) and 1s for voxels out of the brain (outside) -- To do this, we need to make a combination volume of wm and gm (1's for both, everything else 0) called inside, and then create a mirror image (0's for both, everything else 1's)\n", "Then create volumes with the distance tranform for the outside labelled dist2o and wm labelled dist2i  "]}, {"cell_type": "code", "execution_count": null, "id": "2ec19381-6047-42f9-beb3-8cdf0ab5bede", "metadata": {"tags": []}, "outputs": [], "source": ["%load \"answers/answer_009.txt\""]}, {"cell_type": "code", "execution_count": null, "id": "ea14f538", "metadata": {}, "outputs": [], "source": ["#look at aeverything\n", "fig, ax = plt.subplots(figsize=(12,8))\n", "ax.imshow(inside[:,:,50],cmap='gray')\n", "fig, ax = plt.subplots(figsize=(12,8))\n", "ax.imshow(outside[:,:,50],cmap='gray')\n", "fig, ax = plt.subplots(figsize=(12,8))\n", "ax.imshow(dist2o[:,:,50],cmap='gray')\n", "fig, ax = plt.subplots(figsize=(12,8))\n", "ax.imshow(dist2i[:,:,50],cmap='gray')"]}, {"cell_type": "markdown", "id": "7c943b7d-a696-44ca-90d2-068f5a05b521", "metadata": {}, "source": ["Now we have 2 volumes dist2o and dist2i which we can use to get a middle segmentation of the grey matter. The middle boundary of the grey matter is where the distance of the outside of the brain to the inside is equal to the distance of the wm to the outside of the brain. BUT, we just want a ~1 voxel thick border so we will get all voxels where dist2o is smaller than dist2i, and then make a new volume midvox that is the distance transform of the the voxels just to the outside"]}, {"cell_type": "code", "execution_count": null, "id": "5d1377c3-6ee3-40e3-9418-37dd17f17ab6", "metadata": {"tags": []}, "outputs": [], "source": ["midseg=wm.astype(np.int16)*0\n", "midseg[dist2o<dist2i]=1;\n", "inmidseg = abs(midseg-1)\n", "midvox=wm.astype(np.int16)*0\n", "midvox[scipy.ndimage.distance_transform_edt(inmidseg)==1]=1\n", "fig, ax = plt.subplots(figsize=(12,8))\n", "ax.imshow(midseg[:,:,45])\n", "fig, ax = plt.subplots(figsize=(12,8))\n", "testseg = neoseg.astype(np.int16)\n", "testseg[testseg>5]=0\n", "ax.imshow(testseg[:,:,45])\n", "ax.imshow(midvox[:,:,45], alpha = .1,cmap='gray')"]}, {"cell_type": "code", "execution_count": null, "id": "11520c3f-5fd7-4a8e-8ddd-c2a1520782e7", "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "id": "cefe06c1", "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "id": "c20323df", "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "id": "3fcee681", "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "id": "f578d9c8", "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "id": "3473f1e9", "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "id": "8a5b8e25", "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "id": "e06090dd", "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "id": "7e7bae77", "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.5"}}, "nbformat": 4, "nbformat_minor": 5}